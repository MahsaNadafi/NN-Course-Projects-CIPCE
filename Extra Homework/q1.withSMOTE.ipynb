{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["RrSIH1tgmbpD","7OZHN8Fmmk8e","_cGVOfE8zX0Y","xlOd8UfIWrB2"],"authorship_tag":"ABX9TyMfeODVXs7tdGENRW3xzAkz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TewEhiBDxhPQ","executionInfo":{"status":"ok","timestamp":1671285380204,"user_tz":-210,"elapsed":26058,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"fd204de7-2610-4a16-e597-ab4c8ef274f2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/NNDL/HW Extra\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/NNDL/HW Extra"]},{"cell_type":"markdown","source":["# load dataset"],"metadata":{"id":"RrSIH1tgmbpD"}},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('creditcard.csv')\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"p6a39ySOmd66","executionInfo":{"status":"ok","timestamp":1671285385999,"user_tz":-210,"elapsed":3517,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"ed627187-eeca-462f-f24b-730401450830"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   Time        V1        V2        V3        V4        V5        V6        V7  \\\n","0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n","1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n","2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n","3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n","4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n","\n","         V8        V9  ...       V21       V22       V23       V24       V25  \\\n","0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n","1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n","2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n","3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n","4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n","\n","        V26       V27       V28  Amount  Class  \n","0 -0.189115  0.133558 -0.021053  149.62      0  \n","1  0.125895 -0.008983  0.014724    2.69      0  \n","2 -0.139097 -0.055353 -0.059752  378.66      0  \n","3 -0.221929  0.062723  0.061458  123.50      0  \n","4  0.502292  0.219422  0.215153   69.99      0  \n","\n","[5 rows x 31 columns]"],"text/html":["\n","  <div id=\"df-e4733f79-1359-44b8-9288-8e9590c39df1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Time</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Amount</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>-1.359807</td>\n","      <td>-0.072781</td>\n","      <td>2.536347</td>\n","      <td>1.378155</td>\n","      <td>-0.338321</td>\n","      <td>0.462388</td>\n","      <td>0.239599</td>\n","      <td>0.098698</td>\n","      <td>0.363787</td>\n","      <td>...</td>\n","      <td>-0.018307</td>\n","      <td>0.277838</td>\n","      <td>-0.110474</td>\n","      <td>0.066928</td>\n","      <td>0.128539</td>\n","      <td>-0.189115</td>\n","      <td>0.133558</td>\n","      <td>-0.021053</td>\n","      <td>149.62</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.0</td>\n","      <td>1.191857</td>\n","      <td>0.266151</td>\n","      <td>0.166480</td>\n","      <td>0.448154</td>\n","      <td>0.060018</td>\n","      <td>-0.082361</td>\n","      <td>-0.078803</td>\n","      <td>0.085102</td>\n","      <td>-0.255425</td>\n","      <td>...</td>\n","      <td>-0.225775</td>\n","      <td>-0.638672</td>\n","      <td>0.101288</td>\n","      <td>-0.339846</td>\n","      <td>0.167170</td>\n","      <td>0.125895</td>\n","      <td>-0.008983</td>\n","      <td>0.014724</td>\n","      <td>2.69</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1.0</td>\n","      <td>-1.358354</td>\n","      <td>-1.340163</td>\n","      <td>1.773209</td>\n","      <td>0.379780</td>\n","      <td>-0.503198</td>\n","      <td>1.800499</td>\n","      <td>0.791461</td>\n","      <td>0.247676</td>\n","      <td>-1.514654</td>\n","      <td>...</td>\n","      <td>0.247998</td>\n","      <td>0.771679</td>\n","      <td>0.909412</td>\n","      <td>-0.689281</td>\n","      <td>-0.327642</td>\n","      <td>-0.139097</td>\n","      <td>-0.055353</td>\n","      <td>-0.059752</td>\n","      <td>378.66</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1.0</td>\n","      <td>-0.966272</td>\n","      <td>-0.185226</td>\n","      <td>1.792993</td>\n","      <td>-0.863291</td>\n","      <td>-0.010309</td>\n","      <td>1.247203</td>\n","      <td>0.237609</td>\n","      <td>0.377436</td>\n","      <td>-1.387024</td>\n","      <td>...</td>\n","      <td>-0.108300</td>\n","      <td>0.005274</td>\n","      <td>-0.190321</td>\n","      <td>-1.175575</td>\n","      <td>0.647376</td>\n","      <td>-0.221929</td>\n","      <td>0.062723</td>\n","      <td>0.061458</td>\n","      <td>123.50</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2.0</td>\n","      <td>-1.158233</td>\n","      <td>0.877737</td>\n","      <td>1.548718</td>\n","      <td>0.403034</td>\n","      <td>-0.407193</td>\n","      <td>0.095921</td>\n","      <td>0.592941</td>\n","      <td>-0.270533</td>\n","      <td>0.817739</td>\n","      <td>...</td>\n","      <td>-0.009431</td>\n","      <td>0.798278</td>\n","      <td>-0.137458</td>\n","      <td>0.141267</td>\n","      <td>-0.206010</td>\n","      <td>0.502292</td>\n","      <td>0.219422</td>\n","      <td>0.215153</td>\n","      <td>69.99</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 31 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e4733f79-1359-44b8-9288-8e9590c39df1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e4733f79-1359-44b8-9288-8e9590c39df1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e4733f79-1359-44b8-9288-8e9590c39df1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# preprocces"],"metadata":{"id":"7OZHN8Fmmk8e"}},{"cell_type":"markdown","source":["scale amount and drop time"],"metadata":{"id":"ceEsShormpZ2"}},{"cell_type":"code","source":["from sklearn.preprocessing import RobustScaler, StandardScaler, MinMaxScaler\n","\n","#scaler = StandardScaler()\n","scaler = MinMaxScaler()\n","\n","df['scaled_amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n","\n","df.drop(['Time','Amount'], axis=1, inplace=True)"],"metadata":{"id":"p8SGoVkymoU5","executionInfo":{"status":"ok","timestamp":1671285386597,"user_tz":-210,"elapsed":607,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["scaled_amount = df['scaled_amount']\n","\n","df.drop(['scaled_amount'], axis=1, inplace=True)\n","df.insert(0, 'scaled_amount', scaled_amount)\n","\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"ASjfANEOmj-A","executionInfo":{"status":"ok","timestamp":1671285386598,"user_tz":-210,"elapsed":11,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"a24cea75-2a46-4845-f032-de7aacf59c80"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   scaled_amount        V1        V2        V3        V4        V5        V6  \\\n","0       0.005824 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n","1       0.000105  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n","2       0.014739 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n","3       0.004807 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n","4       0.002724 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n","\n","         V7        V8        V9  ...       V20       V21       V22       V23  \\\n","0  0.239599  0.098698  0.363787  ...  0.251412 -0.018307  0.277838 -0.110474   \n","1 -0.078803  0.085102 -0.255425  ... -0.069083 -0.225775 -0.638672  0.101288   \n","2  0.791461  0.247676 -1.514654  ...  0.524980  0.247998  0.771679  0.909412   \n","3  0.237609  0.377436 -1.387024  ... -0.208038 -0.108300  0.005274 -0.190321   \n","4  0.592941 -0.270533  0.817739  ...  0.408542 -0.009431  0.798278 -0.137458   \n","\n","        V24       V25       V26       V27       V28  Class  \n","0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0  \n","1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0  \n","2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0  \n","3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0  \n","4  0.141267 -0.206010  0.502292  0.219422  0.215153      0  \n","\n","[5 rows x 30 columns]"],"text/html":["\n","  <div id=\"df-88020e81-02d2-41d2-81c9-d2d42313529f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>scaled_amount</th>\n","      <th>V1</th>\n","      <th>V2</th>\n","      <th>V3</th>\n","      <th>V4</th>\n","      <th>V5</th>\n","      <th>V6</th>\n","      <th>V7</th>\n","      <th>V8</th>\n","      <th>V9</th>\n","      <th>...</th>\n","      <th>V20</th>\n","      <th>V21</th>\n","      <th>V22</th>\n","      <th>V23</th>\n","      <th>V24</th>\n","      <th>V25</th>\n","      <th>V26</th>\n","      <th>V27</th>\n","      <th>V28</th>\n","      <th>Class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.005824</td>\n","      <td>-1.359807</td>\n","      <td>-0.072781</td>\n","      <td>2.536347</td>\n","      <td>1.378155</td>\n","      <td>-0.338321</td>\n","      <td>0.462388</td>\n","      <td>0.239599</td>\n","      <td>0.098698</td>\n","      <td>0.363787</td>\n","      <td>...</td>\n","      <td>0.251412</td>\n","      <td>-0.018307</td>\n","      <td>0.277838</td>\n","      <td>-0.110474</td>\n","      <td>0.066928</td>\n","      <td>0.128539</td>\n","      <td>-0.189115</td>\n","      <td>0.133558</td>\n","      <td>-0.021053</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.000105</td>\n","      <td>1.191857</td>\n","      <td>0.266151</td>\n","      <td>0.166480</td>\n","      <td>0.448154</td>\n","      <td>0.060018</td>\n","      <td>-0.082361</td>\n","      <td>-0.078803</td>\n","      <td>0.085102</td>\n","      <td>-0.255425</td>\n","      <td>...</td>\n","      <td>-0.069083</td>\n","      <td>-0.225775</td>\n","      <td>-0.638672</td>\n","      <td>0.101288</td>\n","      <td>-0.339846</td>\n","      <td>0.167170</td>\n","      <td>0.125895</td>\n","      <td>-0.008983</td>\n","      <td>0.014724</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.014739</td>\n","      <td>-1.358354</td>\n","      <td>-1.340163</td>\n","      <td>1.773209</td>\n","      <td>0.379780</td>\n","      <td>-0.503198</td>\n","      <td>1.800499</td>\n","      <td>0.791461</td>\n","      <td>0.247676</td>\n","      <td>-1.514654</td>\n","      <td>...</td>\n","      <td>0.524980</td>\n","      <td>0.247998</td>\n","      <td>0.771679</td>\n","      <td>0.909412</td>\n","      <td>-0.689281</td>\n","      <td>-0.327642</td>\n","      <td>-0.139097</td>\n","      <td>-0.055353</td>\n","      <td>-0.059752</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.004807</td>\n","      <td>-0.966272</td>\n","      <td>-0.185226</td>\n","      <td>1.792993</td>\n","      <td>-0.863291</td>\n","      <td>-0.010309</td>\n","      <td>1.247203</td>\n","      <td>0.237609</td>\n","      <td>0.377436</td>\n","      <td>-1.387024</td>\n","      <td>...</td>\n","      <td>-0.208038</td>\n","      <td>-0.108300</td>\n","      <td>0.005274</td>\n","      <td>-0.190321</td>\n","      <td>-1.175575</td>\n","      <td>0.647376</td>\n","      <td>-0.221929</td>\n","      <td>0.062723</td>\n","      <td>0.061458</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.002724</td>\n","      <td>-1.158233</td>\n","      <td>0.877737</td>\n","      <td>1.548718</td>\n","      <td>0.403034</td>\n","      <td>-0.407193</td>\n","      <td>0.095921</td>\n","      <td>0.592941</td>\n","      <td>-0.270533</td>\n","      <td>0.817739</td>\n","      <td>...</td>\n","      <td>0.408542</td>\n","      <td>-0.009431</td>\n","      <td>0.798278</td>\n","      <td>-0.137458</td>\n","      <td>0.141267</td>\n","      <td>-0.206010</td>\n","      <td>0.502292</td>\n","      <td>0.219422</td>\n","      <td>0.215153</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 30 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88020e81-02d2-41d2-81c9-d2d42313529f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-88020e81-02d2-41d2-81c9-d2d42313529f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-88020e81-02d2-41d2-81c9-d2d42313529f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["train test val"],"metadata":{"id":"EOfUIw5knFlp"}},{"cell_type":"code","source":["import numpy as np\n","\n","\n","X = df.drop('Class', axis=1)\n","y = df['Class']\n","print('Shape of X: {}'.format(X.shape))\n","print('Shape of y: {}'.format(y.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVmISRtonK4d","executionInfo":{"status":"ok","timestamp":1671285386598,"user_tz":-210,"elapsed":9,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"83e15413-5fe0-4182-f7b4-68fbecc6da80"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of X: (284807, 29)\n","Shape of y: (284807,)\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","\n","print(\"Number transactions X_train dataset: \", X_train.shape)\n","print(\"Number transactions y_train dataset: \", y_train.shape)\n","print(\"Number transactions X_test dataset: \", X_test.shape)\n","print(\"Number transactions y_test dataset: \", y_test.shape)\n","print(\"Number transactions X_val dataset: \", X_val.shape)\n","print(\"Number transactions y_val dataset: \", y_val.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Jl_03jOnHSg","executionInfo":{"status":"ok","timestamp":1671285387015,"user_tz":-210,"elapsed":425,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"83bbc3df-ed91-4d58-e403-56a49f190a95"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Number transactions X_train dataset:  (182276, 29)\n","Number transactions y_train dataset:  (182276,)\n","Number transactions X_test dataset:  (56962, 29)\n","Number transactions y_test dataset:  (56962,)\n","Number transactions X_val dataset:  (45569, 29)\n","Number transactions y_val dataset:  (45569,)\n"]}]},{"cell_type":"markdown","source":["**SMOTE**"],"metadata":{"id":"BakdHfTNAPkl"}},{"cell_type":"code","source":["# check version number\n","import imblearn\n","print(imblearn.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SlU_4oJJC-vi","executionInfo":{"status":"ok","timestamp":1671285387461,"user_tz":-210,"elapsed":451,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"9440ac26-e678-408a-bcd5-0857c1595b69"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["0.8.1\n"]}]},{"cell_type":"code","source":["from imblearn.over_sampling import SMOTE\n","\n","print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n","print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n","\n","sm = SMOTE(sampling_strategy=0.5)\n","X_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n","\n","print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n","print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n","\n","print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n","print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vhvJdPBczGza","executionInfo":{"status":"ok","timestamp":1671285388942,"user_tz":-210,"elapsed":1483,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"99d72972-5e55-4fcf-bcfa-4065bcc1448d"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Before OverSampling, counts of label '1': 306\n","Before OverSampling, counts of label '0': 181970 \n","\n","After OverSampling, the shape of train_X: (272955, 29)\n","After OverSampling, the shape of train_y: (272955,) \n","\n","After OverSampling, counts of label '1': 90985\n","After OverSampling, counts of label '0': 181970\n"]}]},{"cell_type":"markdown","source":["one-hot encoding y"],"metadata":{"id":"DpTEeECHnZW4"}},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical\n","\n","y_train = to_categorical(y_train, num_classes = 2)\n","y_train_res = to_categorical(y_train_res, num_classes = 2)\n","y_test = to_categorical(y_test, num_classes = 2)\n","y_val = to_categorical(y_val, num_classes = 2)\n","print(y_train[0 : 2, :])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4gPK_UTnbnT","executionInfo":{"status":"ok","timestamp":1671285391145,"user_tz":-210,"elapsed":2207,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"a8e11bb1-0035-4152-9b9c-e4216562502c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1. 0.]\n"," [1. 0.]]\n"]}]},{"cell_type":"markdown","source":["# Denoising autoencoder"],"metadata":{"id":"_cGVOfE8zX0Y"}},{"cell_type":"markdown","source":["making a noisy X_train!"],"metadata":{"id":"IH5EnBgPAVih"}},{"cell_type":"code","source":["import numpy as np \n","\n","clean_signal = X_train_res\n","mu, sigma = 0, 0.1 \n","noise = np.random.normal(mu, sigma, [X_train_res.shape[0],X_train_res.shape[1]])  #gaussian noise\n","noise_signal = clean_signal + noise\n","print(noise_signal.shape)\n","\n","noise_signal_train, noise_signal_val, clean_signal_train, clean_signal_val = train_test_split(noise_signal, clean_signal, test_size=0.2, random_state=0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83KkZyB2_1ne","executionInfo":{"status":"ok","timestamp":1671199618541,"user_tz":-210,"elapsed":562,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"d7d7d88b-e4eb-4a57-e3d9-de3d89c69045"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(272955, 29)\n"]}]},{"cell_type":"markdown","source":["model denoising ae"],"metadata":{"id":"MIJjZwWmAbER"}},{"cell_type":"code","source":["from tensorflow.keras.layers import Dense, Input \n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback, TensorBoard"],"metadata":{"id":"i5hmDdfSzZ1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inp_layer = Input(shape = (29,))\n","hidden_layer1 = Dense(units = 22, activation = 'relu')(inp_layer)\n","hidden_layer2 = Dense(units = 15, activation = 'relu')(hidden_layer1)\n","hidden_layer3 = Dense(units = 10, activation = 'relu')(hidden_layer2)\n","hidden_layer4 = Dense(units = 15, activation = 'relu')(hidden_layer3)\n","hidden_layer5 = Dense(units = 22, activation = 'relu')(hidden_layer4)\n","out_layer = Dense(units = 29, activation = 'linear')(hidden_layer5)\n","\n","Denoising_autoencoder = Model(inputs = inp_layer, outputs = out_layer)\n","Denoising_autoencoder.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EK4VQapGzhBt","executionInfo":{"status":"ok","timestamp":1671199668230,"user_tz":-210,"elapsed":582,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"b667041e-19ba-45fd-afbf-33103e9eedaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 29)]              0         \n","                                                                 \n"," dense (Dense)               (None, 22)                660       \n","                                                                 \n"," dense_1 (Dense)             (None, 15)                345       \n","                                                                 \n"," dense_2 (Dense)             (None, 10)                160       \n","                                                                 \n"," dense_3 (Dense)             (None, 15)                165       \n","                                                                 \n"," dense_4 (Dense)             (None, 22)                352       \n","                                                                 \n"," dense_5 (Dense)             (None, 29)                667       \n","                                                                 \n","=================================================================\n","Total params: 2,349\n","Trainable params: 2,349\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.utils import plot_model\n","plot_model(Denoising_autoencoder, show_shapes=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":754},"id":"lVGCBeGv_qxt","executionInfo":{"status":"ok","timestamp":1671196289958,"user_tz":-210,"elapsed":1235,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"fd27e42a-7cc2-419c-8607-778e6f3227db"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"image/png":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["import h5py\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n","mc = ModelCheckpoint('best_model_AutoEncoder.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n"],"metadata":{"id":"q41S5BKEADD4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import mse\n","\n","opt = Adam(learning_rate=0.001)\n","loss = mse\n","Denoising_autoencoder.compile(optimizer=opt, loss = loss, metrics = 'acc')"],"metadata":{"id":"sFf0hsOh_skC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_Denoising = Denoising_autoencoder.fit(noise_signal_train, clean_signal_train,\n","                            batch_size=1024,\n","                            epochs=1000,\n","                            shuffle=True,\n","                            validation_data = (noise_signal_val, clean_signal_val),\n","                            callbacks=[es, mc])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IiBnxhId_uXD","executionInfo":{"status":"ok","timestamp":1671201057927,"user_tz":-210,"elapsed":1374769,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"08432541-c59c-4765-d699-ebf58d49e6d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","211/214 [============================>.] - ETA: 0s - loss: 3.6607 - acc: 0.1724\n","Epoch 1: val_loss improved from inf to 1.42113, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 3s 9ms/step - loss: 3.6379 - acc: 0.1746 - val_loss: 1.4211 - val_acc: 0.3722\n","Epoch 2/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.8847 - acc: 0.4058\n","Epoch 2: val_loss improved from 1.42113 to 0.64039, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.8778 - acc: 0.4072 - val_loss: 0.6404 - val_acc: 0.4503\n","Epoch 3/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.5994 - acc: 0.4893\n","Epoch 3: val_loss improved from 0.64039 to 0.55680, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.5994 - acc: 0.4893 - val_loss: 0.5568 - val_acc: 0.5238\n","Epoch 4/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.5354 - acc: 0.5472\n","Epoch 4: val_loss improved from 0.55680 to 0.50616, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.5342 - acc: 0.5479 - val_loss: 0.5062 - val_acc: 0.5607\n","Epoch 5/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.4880 - acc: 0.5740\n","Epoch 5: val_loss improved from 0.50616 to 0.46549, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.4876 - acc: 0.5741 - val_loss: 0.4655 - val_acc: 0.5792\n","Epoch 6/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.4502 - acc: 0.5896\n","Epoch 6: val_loss improved from 0.46549 to 0.43231, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.4501 - acc: 0.5897 - val_loss: 0.4323 - val_acc: 0.5987\n","Epoch 7/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.4199 - acc: 0.6112\n","Epoch 7: val_loss improved from 0.43231 to 0.40546, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.4199 - acc: 0.6112 - val_loss: 0.4055 - val_acc: 0.6181\n","Epoch 8/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.3947 - acc: 0.6280\n","Epoch 8: val_loss improved from 0.40546 to 0.38061, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.3944 - acc: 0.6283 - val_loss: 0.3806 - val_acc: 0.6343\n","Epoch 9/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.3677 - acc: 0.6456\n","Epoch 9: val_loss improved from 0.38061 to 0.35454, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.3676 - acc: 0.6456 - val_loss: 0.3545 - val_acc: 0.6541\n","Epoch 10/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.6636\n","Epoch 10: val_loss improved from 0.35454 to 0.33651, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.3457 - acc: 0.6635 - val_loss: 0.3365 - val_acc: 0.6748\n","Epoch 11/1000\n","204/214 [===========================>..] - ETA: 0s - loss: 0.3292 - acc: 0.6787\n","Epoch 11: val_loss improved from 0.33651 to 0.32051, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.3288 - acc: 0.6785 - val_loss: 0.3205 - val_acc: 0.6831\n","Epoch 12/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.3144 - acc: 0.6889\n","Epoch 12: val_loss improved from 0.32051 to 0.30693, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.3141 - acc: 0.6891 - val_loss: 0.3069 - val_acc: 0.6858\n","Epoch 13/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.3007 - acc: 0.6992\n","Epoch 13: val_loss improved from 0.30693 to 0.29424, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.3006 - acc: 0.6993 - val_loss: 0.2942 - val_acc: 0.7022\n","Epoch 14/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.2881 - acc: 0.7102\n","Epoch 14: val_loss improved from 0.29424 to 0.28282, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2880 - acc: 0.7101 - val_loss: 0.2828 - val_acc: 0.7106\n","Epoch 15/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.2774 - acc: 0.7184\n","Epoch 15: val_loss improved from 0.28282 to 0.27307, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2774 - acc: 0.7184 - val_loss: 0.2731 - val_acc: 0.7223\n","Epoch 16/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.2684 - acc: 0.7236\n","Epoch 16: val_loss improved from 0.27307 to 0.26535, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2684 - acc: 0.7236 - val_loss: 0.2654 - val_acc: 0.7254\n","Epoch 17/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.2613 - acc: 0.7275\n","Epoch 17: val_loss improved from 0.26535 to 0.25783, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2613 - acc: 0.7275 - val_loss: 0.2578 - val_acc: 0.7284\n","Epoch 18/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.2553 - acc: 0.7301\n","Epoch 18: val_loss improved from 0.25783 to 0.25408, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2552 - acc: 0.7302 - val_loss: 0.2541 - val_acc: 0.7336\n","Epoch 19/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.2499 - acc: 0.7333\n","Epoch 19: val_loss improved from 0.25408 to 0.24786, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2497 - acc: 0.7330 - val_loss: 0.2479 - val_acc: 0.7358\n","Epoch 20/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.2453 - acc: 0.7355\n","Epoch 20: val_loss improved from 0.24786 to 0.24388, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2453 - acc: 0.7355 - val_loss: 0.2439 - val_acc: 0.7353\n","Epoch 21/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.2411 - acc: 0.7388\n","Epoch 21: val_loss improved from 0.24388 to 0.23935, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2412 - acc: 0.7390 - val_loss: 0.2393 - val_acc: 0.7412\n","Epoch 22/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.2376 - acc: 0.7424\n","Epoch 22: val_loss improved from 0.23935 to 0.23521, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2372 - acc: 0.7423 - val_loss: 0.2352 - val_acc: 0.7442\n","Epoch 23/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.7477\n","Epoch 23: val_loss improved from 0.23521 to 0.23150, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2329 - acc: 0.7478 - val_loss: 0.2315 - val_acc: 0.7491\n","Epoch 24/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.2291 - acc: 0.7533\n","Epoch 24: val_loss improved from 0.23150 to 0.22828, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2292 - acc: 0.7531 - val_loss: 0.2283 - val_acc: 0.7533\n","Epoch 25/1000\n","204/214 [===========================>..] - ETA: 0s - loss: 0.2261 - acc: 0.7580\n","Epoch 25: val_loss improved from 0.22828 to 0.22741, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2260 - acc: 0.7579 - val_loss: 0.2274 - val_acc: 0.7594\n","Epoch 26/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.7625\n","Epoch 26: val_loss improved from 0.22741 to 0.22281, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2236 - acc: 0.7624 - val_loss: 0.2228 - val_acc: 0.7609\n","Epoch 27/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.2211 - acc: 0.7658\n","Epoch 27: val_loss improved from 0.22281 to 0.22038, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2211 - acc: 0.7658 - val_loss: 0.2204 - val_acc: 0.7652\n","Epoch 28/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.2189 - acc: 0.7683\n","Epoch 28: val_loss improved from 0.22038 to 0.21864, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.2189 - acc: 0.7684 - val_loss: 0.2186 - val_acc: 0.7669\n","Epoch 29/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.2169 - acc: 0.7712\n","Epoch 29: val_loss improved from 0.21864 to 0.21676, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.2169 - acc: 0.7711 - val_loss: 0.2168 - val_acc: 0.7712\n","Epoch 30/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.2150 - acc: 0.7721\n","Epoch 30: val_loss improved from 0.21676 to 0.21486, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.2150 - acc: 0.7721 - val_loss: 0.2149 - val_acc: 0.7727\n","Epoch 31/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.7735\n","Epoch 31: val_loss improved from 0.21486 to 0.21349, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.2136 - acc: 0.7736 - val_loss: 0.2135 - val_acc: 0.7751\n","Epoch 32/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.2118 - acc: 0.7745\n","Epoch 32: val_loss improved from 0.21349 to 0.21273, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2120 - acc: 0.7746 - val_loss: 0.2127 - val_acc: 0.7746\n","Epoch 33/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.2108 - acc: 0.7753\n","Epoch 33: val_loss improved from 0.21273 to 0.21177, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2108 - acc: 0.7754 - val_loss: 0.2118 - val_acc: 0.7738\n","Epoch 34/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.2095 - acc: 0.7761\n","Epoch 34: val_loss improved from 0.21177 to 0.21014, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2095 - acc: 0.7761 - val_loss: 0.2101 - val_acc: 0.7746\n","Epoch 35/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.2085 - acc: 0.7761\n","Epoch 35: val_loss improved from 0.21014 to 0.20958, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2085 - acc: 0.7761 - val_loss: 0.2096 - val_acc: 0.7772\n","Epoch 36/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.2076 - acc: 0.7773\n","Epoch 36: val_loss improved from 0.20958 to 0.20850, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2075 - acc: 0.7775 - val_loss: 0.2085 - val_acc: 0.7755\n","Epoch 37/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.2065 - acc: 0.7776\n","Epoch 37: val_loss improved from 0.20850 to 0.20742, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2064 - acc: 0.7776 - val_loss: 0.2074 - val_acc: 0.7770\n","Epoch 38/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.2056 - acc: 0.7784\n","Epoch 38: val_loss did not improve from 0.20742\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2056 - acc: 0.7784 - val_loss: 0.2074 - val_acc: 0.7750\n","Epoch 39/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.2050 - acc: 0.7781\n","Epoch 39: val_loss improved from 0.20742 to 0.20601, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2050 - acc: 0.7782 - val_loss: 0.2060 - val_acc: 0.7745\n","Epoch 40/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.2039 - acc: 0.7789\n","Epoch 40: val_loss improved from 0.20601 to 0.20505, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2041 - acc: 0.7789 - val_loss: 0.2050 - val_acc: 0.7777\n","Epoch 41/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.2037 - acc: 0.7793\n","Epoch 41: val_loss did not improve from 0.20505\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2036 - acc: 0.7790 - val_loss: 0.2051 - val_acc: 0.7756\n","Epoch 42/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.2032 - acc: 0.7784\n","Epoch 42: val_loss improved from 0.20505 to 0.20447, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2029 - acc: 0.7788 - val_loss: 0.2045 - val_acc: 0.7758\n","Epoch 43/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.2024 - acc: 0.7795\n","Epoch 43: val_loss improved from 0.20447 to 0.20431, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2024 - acc: 0.7795 - val_loss: 0.2043 - val_acc: 0.7772\n","Epoch 44/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.2018 - acc: 0.7795\n","Epoch 44: val_loss improved from 0.20431 to 0.20245, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2018 - acc: 0.7795 - val_loss: 0.2024 - val_acc: 0.7777\n","Epoch 45/1000\n","204/214 [===========================>..] - ETA: 0s - loss: 0.2016 - acc: 0.7796\n","Epoch 45: val_loss did not improve from 0.20245\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2014 - acc: 0.7797 - val_loss: 0.2028 - val_acc: 0.7753\n","Epoch 46/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.2008 - acc: 0.7800\n","Epoch 46: val_loss did not improve from 0.20245\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2008 - acc: 0.7799 - val_loss: 0.2026 - val_acc: 0.7768\n","Epoch 47/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.2004 - acc: 0.7801\n","Epoch 47: val_loss improved from 0.20245 to 0.20165, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.2003 - acc: 0.7801 - val_loss: 0.2017 - val_acc: 0.7806\n","Epoch 48/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.2000 - acc: 0.7801\n","Epoch 48: val_loss improved from 0.20165 to 0.20099, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1998 - acc: 0.7803 - val_loss: 0.2010 - val_acc: 0.7778\n","Epoch 49/1000\n","204/214 [===========================>..] - ETA: 0s - loss: 0.1993 - acc: 0.7803\n","Epoch 49: val_loss did not improve from 0.20099\n","214/214 [==============================] - 1s 5ms/step - loss: 0.1994 - acc: 0.7803 - val_loss: 0.2011 - val_acc: 0.7785\n","Epoch 50/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1989 - acc: 0.7806\n","Epoch 50: val_loss improved from 0.20099 to 0.19954, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1989 - acc: 0.7807 - val_loss: 0.1995 - val_acc: 0.7785\n","Epoch 51/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1985 - acc: 0.7805\n","Epoch 51: val_loss did not improve from 0.19954\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1985 - acc: 0.7806 - val_loss: 0.2001 - val_acc: 0.7792\n","Epoch 52/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.7805\n","Epoch 52: val_loss improved from 0.19954 to 0.19947, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1981 - acc: 0.7805 - val_loss: 0.1995 - val_acc: 0.7771\n","Epoch 53/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1975 - acc: 0.7811\n","Epoch 53: val_loss improved from 0.19947 to 0.19845, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1976 - acc: 0.7811 - val_loss: 0.1984 - val_acc: 0.7798\n","Epoch 54/1000\n","204/214 [===========================>..] - ETA: 0s - loss: 0.1974 - acc: 0.7815\n","Epoch 54: val_loss did not improve from 0.19845\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1974 - acc: 0.7816 - val_loss: 0.2008 - val_acc: 0.7787\n","Epoch 55/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1970 - acc: 0.7818\n","Epoch 55: val_loss improved from 0.19845 to 0.19844, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1970 - acc: 0.7818 - val_loss: 0.1984 - val_acc: 0.7794\n","Epoch 56/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1965 - acc: 0.7817\n","Epoch 56: val_loss improved from 0.19844 to 0.19735, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1965 - acc: 0.7817 - val_loss: 0.1973 - val_acc: 0.7795\n","Epoch 57/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1962 - acc: 0.7817\n","Epoch 57: val_loss improved from 0.19735 to 0.19713, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1962 - acc: 0.7816 - val_loss: 0.1971 - val_acc: 0.7795\n","Epoch 58/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1956 - acc: 0.7824\n","Epoch 58: val_loss improved from 0.19713 to 0.19712, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1959 - acc: 0.7820 - val_loss: 0.1971 - val_acc: 0.7786\n","Epoch 59/1000\n","204/214 [===========================>..] - ETA: 0s - loss: 0.1954 - acc: 0.7828\n","Epoch 59: val_loss improved from 0.19712 to 0.19658, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1954 - acc: 0.7828 - val_loss: 0.1966 - val_acc: 0.7784\n","Epoch 60/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1952 - acc: 0.7822\n","Epoch 60: val_loss improved from 0.19658 to 0.19638, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1951 - acc: 0.7823 - val_loss: 0.1964 - val_acc: 0.7789\n","Epoch 61/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1947 - acc: 0.7828\n","Epoch 61: val_loss did not improve from 0.19638\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1947 - acc: 0.7828 - val_loss: 0.1967 - val_acc: 0.7794\n","Epoch 62/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1944 - acc: 0.7825\n","Epoch 62: val_loss improved from 0.19638 to 0.19577, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1944 - acc: 0.7825 - val_loss: 0.1958 - val_acc: 0.7792\n","Epoch 63/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1943 - acc: 0.7827\n","Epoch 63: val_loss did not improve from 0.19577\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1943 - acc: 0.7828 - val_loss: 0.1964 - val_acc: 0.7788\n","Epoch 64/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1937 - acc: 0.7834\n","Epoch 64: val_loss improved from 0.19577 to 0.19546, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1937 - acc: 0.7834 - val_loss: 0.1955 - val_acc: 0.7792\n","Epoch 65/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1935 - acc: 0.7834\n","Epoch 65: val_loss improved from 0.19546 to 0.19444, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1935 - acc: 0.7833 - val_loss: 0.1944 - val_acc: 0.7802\n","Epoch 66/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.1930 - acc: 0.7829\n","Epoch 66: val_loss improved from 0.19444 to 0.19378, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1931 - acc: 0.7830 - val_loss: 0.1938 - val_acc: 0.7827\n","Epoch 67/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1925 - acc: 0.7835\n","Epoch 67: val_loss improved from 0.19378 to 0.19357, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1926 - acc: 0.7832 - val_loss: 0.1936 - val_acc: 0.7814\n","Epoch 68/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1923 - acc: 0.7833\n","Epoch 68: val_loss improved from 0.19357 to 0.19337, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1923 - acc: 0.7829 - val_loss: 0.1934 - val_acc: 0.7802\n","Epoch 69/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1919 - acc: 0.7826\n","Epoch 69: val_loss improved from 0.19337 to 0.19289, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1919 - acc: 0.7826 - val_loss: 0.1929 - val_acc: 0.7800\n","Epoch 70/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1914 - acc: 0.7827\n","Epoch 70: val_loss did not improve from 0.19289\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1914 - acc: 0.7827 - val_loss: 0.1936 - val_acc: 0.7758\n","Epoch 71/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1911 - acc: 0.7826\n","Epoch 71: val_loss improved from 0.19289 to 0.19173, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1911 - acc: 0.7827 - val_loss: 0.1917 - val_acc: 0.7795\n","Epoch 72/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1905 - acc: 0.7825\n","Epoch 72: val_loss improved from 0.19173 to 0.19082, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1906 - acc: 0.7824 - val_loss: 0.1908 - val_acc: 0.7809\n","Epoch 73/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1900 - acc: 0.7820\n","Epoch 73: val_loss did not improve from 0.19082\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1900 - acc: 0.7820 - val_loss: 0.1910 - val_acc: 0.7810\n","Epoch 74/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1896 - acc: 0.7823\n","Epoch 74: val_loss improved from 0.19082 to 0.19036, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1896 - acc: 0.7822 - val_loss: 0.1904 - val_acc: 0.7814\n","Epoch 75/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.7816\n","Epoch 75: val_loss improved from 0.19036 to 0.19005, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1892 - acc: 0.7818 - val_loss: 0.1900 - val_acc: 0.7800\n","Epoch 76/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.1884 - acc: 0.7820\n","Epoch 76: val_loss improved from 0.19005 to 0.18889, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1883 - acc: 0.7819 - val_loss: 0.1889 - val_acc: 0.7823\n","Epoch 77/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1877 - acc: 0.7822\n","Epoch 77: val_loss improved from 0.18889 to 0.18818, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1877 - acc: 0.7823 - val_loss: 0.1882 - val_acc: 0.7809\n","Epoch 78/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1873 - acc: 0.7813\n","Epoch 78: val_loss improved from 0.18818 to 0.18790, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1871 - acc: 0.7813 - val_loss: 0.1879 - val_acc: 0.7798\n","Epoch 79/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1868 - acc: 0.7806\n","Epoch 79: val_loss improved from 0.18790 to 0.18756, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1867 - acc: 0.7806 - val_loss: 0.1876 - val_acc: 0.7812\n","Epoch 80/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1860 - acc: 0.7811\n","Epoch 80: val_loss improved from 0.18756 to 0.18683, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1860 - acc: 0.7811 - val_loss: 0.1868 - val_acc: 0.7795\n","Epoch 81/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1850 - acc: 0.7812\n","Epoch 81: val_loss improved from 0.18683 to 0.18557, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1850 - acc: 0.7810 - val_loss: 0.1856 - val_acc: 0.7807\n","Epoch 82/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1846 - acc: 0.7806\n","Epoch 82: val_loss improved from 0.18557 to 0.18499, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1846 - acc: 0.7806 - val_loss: 0.1850 - val_acc: 0.7802\n","Epoch 83/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.1839 - acc: 0.7802\n","Epoch 83: val_loss improved from 0.18499 to 0.18443, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1837 - acc: 0.7803 - val_loss: 0.1844 - val_acc: 0.7804\n","Epoch 84/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1831 - acc: 0.7800\n","Epoch 84: val_loss improved from 0.18443 to 0.18393, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1831 - acc: 0.7801 - val_loss: 0.1839 - val_acc: 0.7812\n","Epoch 85/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1825 - acc: 0.7800\n","Epoch 85: val_loss improved from 0.18393 to 0.18304, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1825 - acc: 0.7801 - val_loss: 0.1830 - val_acc: 0.7792\n","Epoch 86/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1820 - acc: 0.7799\n","Epoch 86: val_loss improved from 0.18304 to 0.18238, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1819 - acc: 0.7800 - val_loss: 0.1824 - val_acc: 0.7806\n","Epoch 87/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1809 - acc: 0.7793\n","Epoch 87: val_loss did not improve from 0.18238\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1813 - acc: 0.7794 - val_loss: 0.1833 - val_acc: 0.7813\n","Epoch 88/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1806 - acc: 0.7799\n","Epoch 88: val_loss improved from 0.18238 to 0.18189, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1806 - acc: 0.7799 - val_loss: 0.1819 - val_acc: 0.7802\n","Epoch 89/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1798 - acc: 0.7792\n","Epoch 89: val_loss improved from 0.18189 to 0.18143, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1800 - acc: 0.7792 - val_loss: 0.1814 - val_acc: 0.7808\n","Epoch 90/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1797 - acc: 0.7786\n","Epoch 90: val_loss improved from 0.18143 to 0.18011, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1797 - acc: 0.7787 - val_loss: 0.1801 - val_acc: 0.7793\n","Epoch 91/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1791 - acc: 0.7787\n","Epoch 91: val_loss improved from 0.18011 to 0.17986, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1791 - acc: 0.7788 - val_loss: 0.1799 - val_acc: 0.7788\n","Epoch 92/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1789 - acc: 0.7784\n","Epoch 92: val_loss improved from 0.17986 to 0.17938, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1787 - acc: 0.7785 - val_loss: 0.1794 - val_acc: 0.7800\n","Epoch 93/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1781 - acc: 0.7790\n","Epoch 93: val_loss improved from 0.17938 to 0.17916, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1780 - acc: 0.7791 - val_loss: 0.1792 - val_acc: 0.7794\n","Epoch 94/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1777 - acc: 0.7784\n","Epoch 94: val_loss improved from 0.17916 to 0.17809, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1776 - acc: 0.7784 - val_loss: 0.1781 - val_acc: 0.7803\n","Epoch 95/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1773 - acc: 0.7782\n","Epoch 95: val_loss improved from 0.17809 to 0.17797, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1772 - acc: 0.7783 - val_loss: 0.1780 - val_acc: 0.7786\n","Epoch 96/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1770 - acc: 0.7773\n","Epoch 96: val_loss improved from 0.17797 to 0.17787, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1768 - acc: 0.7774 - val_loss: 0.1779 - val_acc: 0.7804\n","Epoch 97/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1763 - acc: 0.7776\n","Epoch 97: val_loss improved from 0.17787 to 0.17775, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1763 - acc: 0.7779 - val_loss: 0.1777 - val_acc: 0.7766\n","Epoch 98/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1760 - acc: 0.7781\n","Epoch 98: val_loss improved from 0.17775 to 0.17617, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1758 - acc: 0.7781 - val_loss: 0.1762 - val_acc: 0.7800\n","Epoch 99/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1756 - acc: 0.7779\n","Epoch 99: val_loss improved from 0.17617 to 0.17611, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1756 - acc: 0.7781 - val_loss: 0.1761 - val_acc: 0.7769\n","Epoch 100/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1751 - acc: 0.7774\n","Epoch 100: val_loss did not improve from 0.17611\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1752 - acc: 0.7774 - val_loss: 0.1762 - val_acc: 0.7766\n","Epoch 101/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1746 - acc: 0.7778\n","Epoch 101: val_loss improved from 0.17611 to 0.17578, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1747 - acc: 0.7776 - val_loss: 0.1758 - val_acc: 0.7773\n","Epoch 102/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1745 - acc: 0.7773\n","Epoch 102: val_loss improved from 0.17578 to 0.17552, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1745 - acc: 0.7775 - val_loss: 0.1755 - val_acc: 0.7777\n","Epoch 103/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1740 - acc: 0.7774\n","Epoch 103: val_loss improved from 0.17552 to 0.17546, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1740 - acc: 0.7773 - val_loss: 0.1755 - val_acc: 0.7764\n","Epoch 104/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.7770\n","Epoch 104: val_loss improved from 0.17546 to 0.17513, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1738 - acc: 0.7772 - val_loss: 0.1751 - val_acc: 0.7758\n","Epoch 105/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1735 - acc: 0.7775\n","Epoch 105: val_loss improved from 0.17513 to 0.17392, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1735 - acc: 0.7775 - val_loss: 0.1739 - val_acc: 0.7785\n","Epoch 106/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1735 - acc: 0.7774\n","Epoch 106: val_loss did not improve from 0.17392\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1732 - acc: 0.7771 - val_loss: 0.1744 - val_acc: 0.7750\n","Epoch 107/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1728 - acc: 0.7770\n","Epoch 107: val_loss did not improve from 0.17392\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1729 - acc: 0.7769 - val_loss: 0.1771 - val_acc: 0.7734\n","Epoch 108/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1726 - acc: 0.7768\n","Epoch 108: val_loss did not improve from 0.17392\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1726 - acc: 0.7770 - val_loss: 0.1746 - val_acc: 0.7782\n","Epoch 109/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1724 - acc: 0.7767\n","Epoch 109: val_loss improved from 0.17392 to 0.17374, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1723 - acc: 0.7769 - val_loss: 0.1737 - val_acc: 0.7780\n","Epoch 110/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1718 - acc: 0.7767\n","Epoch 110: val_loss improved from 0.17374 to 0.17252, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1718 - acc: 0.7767 - val_loss: 0.1725 - val_acc: 0.7774\n","Epoch 111/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1714 - acc: 0.7770\n","Epoch 111: val_loss did not improve from 0.17252\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1716 - acc: 0.7768 - val_loss: 0.1732 - val_acc: 0.7783\n","Epoch 112/1000\n","204/214 [===========================>..] - ETA: 0s - loss: 0.1715 - acc: 0.7767\n","Epoch 112: val_loss improved from 0.17252 to 0.17192, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1714 - acc: 0.7766 - val_loss: 0.1719 - val_acc: 0.7756\n","Epoch 113/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1708 - acc: 0.7768\n","Epoch 113: val_loss improved from 0.17192 to 0.17122, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1708 - acc: 0.7768 - val_loss: 0.1712 - val_acc: 0.7803\n","Epoch 114/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1707 - acc: 0.7768\n","Epoch 114: val_loss did not improve from 0.17122\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1708 - acc: 0.7769 - val_loss: 0.1717 - val_acc: 0.7765\n","Epoch 115/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1705 - acc: 0.7766\n","Epoch 115: val_loss did not improve from 0.17122\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1705 - acc: 0.7767 - val_loss: 0.1713 - val_acc: 0.7795\n","Epoch 116/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1701 - acc: 0.7769\n","Epoch 116: val_loss improved from 0.17122 to 0.17026, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1702 - acc: 0.7769 - val_loss: 0.1703 - val_acc: 0.7808\n","Epoch 117/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1699 - acc: 0.7771\n","Epoch 117: val_loss improved from 0.17026 to 0.17025, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1698 - acc: 0.7771 - val_loss: 0.1702 - val_acc: 0.7800\n","Epoch 118/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1697 - acc: 0.7767\n","Epoch 118: val_loss did not improve from 0.17025\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1697 - acc: 0.7767 - val_loss: 0.1705 - val_acc: 0.7764\n","Epoch 119/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1694 - acc: 0.7770\n","Epoch 119: val_loss did not improve from 0.17025\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1694 - acc: 0.7770 - val_loss: 0.1719 - val_acc: 0.7732\n","Epoch 120/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1688 - acc: 0.7772\n","Epoch 120: val_loss improved from 0.17025 to 0.16996, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1690 - acc: 0.7772 - val_loss: 0.1700 - val_acc: 0.7803\n","Epoch 121/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.7773\n","Epoch 121: val_loss improved from 0.16996 to 0.16955, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1688 - acc: 0.7773 - val_loss: 0.1696 - val_acc: 0.7790\n","Epoch 122/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.1686 - acc: 0.7773\n","Epoch 122: val_loss improved from 0.16955 to 0.16887, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1686 - acc: 0.7773 - val_loss: 0.1689 - val_acc: 0.7792\n","Epoch 123/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1681 - acc: 0.7772\n","Epoch 123: val_loss did not improve from 0.16887\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1683 - acc: 0.7772 - val_loss: 0.1705 - val_acc: 0.7780\n","Epoch 124/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1680 - acc: 0.7780\n","Epoch 124: val_loss did not improve from 0.16887\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1680 - acc: 0.7782 - val_loss: 0.1693 - val_acc: 0.7812\n","Epoch 125/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.7780\n","Epoch 125: val_loss did not improve from 0.16887\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1678 - acc: 0.7780 - val_loss: 0.1722 - val_acc: 0.7778\n","Epoch 126/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1677 - acc: 0.7779\n","Epoch 126: val_loss improved from 0.16887 to 0.16826, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1677 - acc: 0.7779 - val_loss: 0.1683 - val_acc: 0.7798\n","Epoch 127/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1673 - acc: 0.7774\n","Epoch 127: val_loss did not improve from 0.16826\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1673 - acc: 0.7776 - val_loss: 0.1701 - val_acc: 0.7764\n","Epoch 128/1000\n","204/214 [===========================>..] - ETA: 0s - loss: 0.1670 - acc: 0.7774\n","Epoch 128: val_loss did not improve from 0.16826\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1672 - acc: 0.7776 - val_loss: 0.1707 - val_acc: 0.7764\n","Epoch 129/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1667 - acc: 0.7787\n","Epoch 129: val_loss improved from 0.16826 to 0.16757, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1668 - acc: 0.7787 - val_loss: 0.1676 - val_acc: 0.7788\n","Epoch 130/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.1665 - acc: 0.7794\n","Epoch 130: val_loss did not improve from 0.16757\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1665 - acc: 0.7795 - val_loss: 0.1683 - val_acc: 0.7772\n","Epoch 131/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.7792\n","Epoch 131: val_loss improved from 0.16757 to 0.16685, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1662 - acc: 0.7791 - val_loss: 0.1668 - val_acc: 0.7793\n","Epoch 132/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1661 - acc: 0.7794\n","Epoch 132: val_loss improved from 0.16685 to 0.16617, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1661 - acc: 0.7794 - val_loss: 0.1662 - val_acc: 0.7831\n","Epoch 133/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1656 - acc: 0.7793\n","Epoch 133: val_loss did not improve from 0.16617\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1657 - acc: 0.7794 - val_loss: 0.1664 - val_acc: 0.7797\n","Epoch 134/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1654 - acc: 0.7799\n","Epoch 134: val_loss improved from 0.16617 to 0.16614, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1654 - acc: 0.7799 - val_loss: 0.1661 - val_acc: 0.7811\n","Epoch 135/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1651 - acc: 0.7796\n","Epoch 135: val_loss improved from 0.16614 to 0.16581, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1651 - acc: 0.7795 - val_loss: 0.1658 - val_acc: 0.7822\n","Epoch 136/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1648 - acc: 0.7804\n","Epoch 136: val_loss improved from 0.16581 to 0.16490, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1649 - acc: 0.7803 - val_loss: 0.1649 - val_acc: 0.7834\n","Epoch 137/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1646 - acc: 0.7804\n","Epoch 137: val_loss did not improve from 0.16490\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1646 - acc: 0.7805 - val_loss: 0.1655 - val_acc: 0.7781\n","Epoch 138/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1643 - acc: 0.7808\n","Epoch 138: val_loss improved from 0.16490 to 0.16408, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1642 - acc: 0.7809 - val_loss: 0.1641 - val_acc: 0.7844\n","Epoch 139/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1641 - acc: 0.7810\n","Epoch 139: val_loss improved from 0.16408 to 0.16383, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1640 - acc: 0.7809 - val_loss: 0.1638 - val_acc: 0.7841\n","Epoch 140/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1635 - acc: 0.7812\n","Epoch 140: val_loss improved from 0.16383 to 0.16373, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1636 - acc: 0.7813 - val_loss: 0.1637 - val_acc: 0.7847\n","Epoch 141/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1635 - acc: 0.7813\n","Epoch 141: val_loss improved from 0.16373 to 0.16323, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1635 - acc: 0.7814 - val_loss: 0.1632 - val_acc: 0.7827\n","Epoch 142/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1633 - acc: 0.7814\n","Epoch 142: val_loss improved from 0.16323 to 0.16317, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1633 - acc: 0.7814 - val_loss: 0.1632 - val_acc: 0.7846\n","Epoch 143/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1627 - acc: 0.7821\n","Epoch 143: val_loss improved from 0.16317 to 0.16256, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1627 - acc: 0.7821 - val_loss: 0.1626 - val_acc: 0.7843\n","Epoch 144/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1626 - acc: 0.7824\n","Epoch 144: val_loss did not improve from 0.16256\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1626 - acc: 0.7825 - val_loss: 0.1631 - val_acc: 0.7837\n","Epoch 145/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1623 - acc: 0.7825\n","Epoch 145: val_loss improved from 0.16256 to 0.16206, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1622 - acc: 0.7827 - val_loss: 0.1621 - val_acc: 0.7847\n","Epoch 146/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1620 - acc: 0.7830\n","Epoch 146: val_loss improved from 0.16206 to 0.16152, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1620 - acc: 0.7829 - val_loss: 0.1615 - val_acc: 0.7870\n","Epoch 147/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1616 - acc: 0.7833\n","Epoch 147: val_loss did not improve from 0.16152\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1617 - acc: 0.7833 - val_loss: 0.1635 - val_acc: 0.7800\n","Epoch 148/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1616 - acc: 0.7833\n","Epoch 148: val_loss improved from 0.16152 to 0.16136, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1616 - acc: 0.7833 - val_loss: 0.1614 - val_acc: 0.7850\n","Epoch 149/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1609 - acc: 0.7842\n","Epoch 149: val_loss did not improve from 0.16136\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1610 - acc: 0.7839 - val_loss: 0.1617 - val_acc: 0.7860\n","Epoch 150/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1612 - acc: 0.7838\n","Epoch 150: val_loss improved from 0.16136 to 0.16127, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1611 - acc: 0.7838 - val_loss: 0.1613 - val_acc: 0.7883\n","Epoch 151/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1605 - acc: 0.7842\n","Epoch 151: val_loss did not improve from 0.16127\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1605 - acc: 0.7841 - val_loss: 0.1617 - val_acc: 0.7826\n","Epoch 152/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1603 - acc: 0.7841\n","Epoch 152: val_loss improved from 0.16127 to 0.16026, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1604 - acc: 0.7841 - val_loss: 0.1603 - val_acc: 0.7889\n","Epoch 153/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1601 - acc: 0.7846\n","Epoch 153: val_loss did not improve from 0.16026\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1601 - acc: 0.7849 - val_loss: 0.1641 - val_acc: 0.7810\n","Epoch 154/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1600 - acc: 0.7847\n","Epoch 154: val_loss improved from 0.16026 to 0.15955, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1600 - acc: 0.7846 - val_loss: 0.1596 - val_acc: 0.7868\n","Epoch 155/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1596 - acc: 0.7848\n","Epoch 155: val_loss improved from 0.15955 to 0.15889, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1596 - acc: 0.7849 - val_loss: 0.1589 - val_acc: 0.7885\n","Epoch 156/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1592 - acc: 0.7857\n","Epoch 156: val_loss did not improve from 0.15889\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1593 - acc: 0.7857 - val_loss: 0.1604 - val_acc: 0.7866\n","Epoch 157/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1593 - acc: 0.7862\n","Epoch 157: val_loss did not improve from 0.15889\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1593 - acc: 0.7862 - val_loss: 0.1609 - val_acc: 0.7817\n","Epoch 158/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1588 - acc: 0.7865\n","Epoch 158: val_loss did not improve from 0.15889\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1589 - acc: 0.7865 - val_loss: 0.1597 - val_acc: 0.7863\n","Epoch 159/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1586 - acc: 0.7870\n","Epoch 159: val_loss improved from 0.15889 to 0.15805, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 3s 14ms/step - loss: 0.1586 - acc: 0.7869 - val_loss: 0.1580 - val_acc: 0.7887\n","Epoch 160/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1585 - acc: 0.7873\n","Epoch 160: val_loss did not improve from 0.15805\n","214/214 [==============================] - 3s 14ms/step - loss: 0.1586 - acc: 0.7873 - val_loss: 0.1599 - val_acc: 0.7842\n","Epoch 161/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1584 - acc: 0.7874\n","Epoch 161: val_loss did not improve from 0.15805\n","214/214 [==============================] - 3s 14ms/step - loss: 0.1584 - acc: 0.7874 - val_loss: 0.1587 - val_acc: 0.7895\n","Epoch 162/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.7876\n","Epoch 162: val_loss improved from 0.15805 to 0.15748, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 3s 15ms/step - loss: 0.1580 - acc: 0.7877 - val_loss: 0.1575 - val_acc: 0.7899\n","Epoch 163/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1580 - acc: 0.7882\n","Epoch 163: val_loss did not improve from 0.15748\n","214/214 [==============================] - 3s 12ms/step - loss: 0.1579 - acc: 0.7883 - val_loss: 0.1581 - val_acc: 0.7888\n","Epoch 164/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1576 - acc: 0.7885\n","Epoch 164: val_loss improved from 0.15748 to 0.15739, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1576 - acc: 0.7885 - val_loss: 0.1574 - val_acc: 0.7903\n","Epoch 165/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1576 - acc: 0.7892\n","Epoch 165: val_loss improved from 0.15739 to 0.15679, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1575 - acc: 0.7892 - val_loss: 0.1568 - val_acc: 0.7896\n","Epoch 166/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1573 - acc: 0.7892\n","Epoch 166: val_loss did not improve from 0.15679\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1573 - acc: 0.7893 - val_loss: 0.1574 - val_acc: 0.7894\n","Epoch 167/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1571 - acc: 0.7892\n","Epoch 167: val_loss improved from 0.15679 to 0.15629, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1573 - acc: 0.7894 - val_loss: 0.1563 - val_acc: 0.7911\n","Epoch 168/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1572 - acc: 0.7897\n","Epoch 168: val_loss did not improve from 0.15629\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1571 - acc: 0.7896 - val_loss: 0.1569 - val_acc: 0.7937\n","Epoch 169/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1569 - acc: 0.7899\n","Epoch 169: val_loss did not improve from 0.15629\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1569 - acc: 0.7901 - val_loss: 0.1569 - val_acc: 0.7922\n","Epoch 170/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1566 - acc: 0.7903\n","Epoch 170: val_loss improved from 0.15629 to 0.15605, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1566 - acc: 0.7904 - val_loss: 0.1560 - val_acc: 0.7917\n","Epoch 171/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1565 - acc: 0.7904\n","Epoch 171: val_loss did not improve from 0.15605\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1565 - acc: 0.7904 - val_loss: 0.1568 - val_acc: 0.7941\n","Epoch 172/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1563 - acc: 0.7912\n","Epoch 172: val_loss improved from 0.15605 to 0.15604, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1563 - acc: 0.7913 - val_loss: 0.1560 - val_acc: 0.7937\n","Epoch 173/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1562 - acc: 0.7914\n","Epoch 173: val_loss did not improve from 0.15604\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1562 - acc: 0.7913 - val_loss: 0.1569 - val_acc: 0.7923\n","Epoch 174/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1560 - acc: 0.7917\n","Epoch 174: val_loss did not improve from 0.15604\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1559 - acc: 0.7918 - val_loss: 0.1563 - val_acc: 0.7893\n","Epoch 175/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1557 - acc: 0.7921\n","Epoch 175: val_loss improved from 0.15604 to 0.15549, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1557 - acc: 0.7921 - val_loss: 0.1555 - val_acc: 0.7945\n","Epoch 176/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1556 - acc: 0.7926\n","Epoch 176: val_loss improved from 0.15549 to 0.15461, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1555 - acc: 0.7927 - val_loss: 0.1546 - val_acc: 0.7949\n","Epoch 177/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1555 - acc: 0.7935\n","Epoch 177: val_loss did not improve from 0.15461\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1554 - acc: 0.7934 - val_loss: 0.1548 - val_acc: 0.7941\n","Epoch 178/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.1551 - acc: 0.7928\n","Epoch 178: val_loss improved from 0.15461 to 0.15460, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1552 - acc: 0.7929 - val_loss: 0.1546 - val_acc: 0.7943\n","Epoch 179/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1552 - acc: 0.7935\n","Epoch 179: val_loss did not improve from 0.15460\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1552 - acc: 0.7934 - val_loss: 0.1550 - val_acc: 0.7934\n","Epoch 180/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.1547 - acc: 0.7938\n","Epoch 180: val_loss did not improve from 0.15460\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1548 - acc: 0.7940 - val_loss: 0.1547 - val_acc: 0.7952\n","Epoch 181/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1546 - acc: 0.7938\n","Epoch 181: val_loss did not improve from 0.15460\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1547 - acc: 0.7942 - val_loss: 0.1555 - val_acc: 0.7940\n","Epoch 182/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1547 - acc: 0.7947\n","Epoch 182: val_loss improved from 0.15460 to 0.15452, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1547 - acc: 0.7947 - val_loss: 0.1545 - val_acc: 0.7936\n","Epoch 183/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.1545 - acc: 0.7947\n","Epoch 183: val_loss did not improve from 0.15452\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1544 - acc: 0.7947 - val_loss: 0.1554 - val_acc: 0.7964\n","Epoch 184/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1543 - acc: 0.7952\n","Epoch 184: val_loss did not improve from 0.15452\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1543 - acc: 0.7952 - val_loss: 0.1546 - val_acc: 0.7961\n","Epoch 185/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1545 - acc: 0.7950\n","Epoch 185: val_loss improved from 0.15452 to 0.15388, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1543 - acc: 0.7948 - val_loss: 0.1539 - val_acc: 0.7933\n","Epoch 186/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1539 - acc: 0.7957\n","Epoch 186: val_loss did not improve from 0.15388\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1540 - acc: 0.7956 - val_loss: 0.1540 - val_acc: 0.7935\n","Epoch 187/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1538 - acc: 0.7962\n","Epoch 187: val_loss improved from 0.15388 to 0.15346, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1538 - acc: 0.7961 - val_loss: 0.1535 - val_acc: 0.7981\n","Epoch 188/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.7960\n","Epoch 188: val_loss did not improve from 0.15346\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1537 - acc: 0.7959 - val_loss: 0.1568 - val_acc: 0.7964\n","Epoch 189/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1536 - acc: 0.7962\n","Epoch 189: val_loss improved from 0.15346 to 0.15292, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1536 - acc: 0.7962 - val_loss: 0.1529 - val_acc: 0.7980\n","Epoch 190/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.7968\n","Epoch 190: val_loss did not improve from 0.15292\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1533 - acc: 0.7969 - val_loss: 0.1536 - val_acc: 0.7967\n","Epoch 191/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1533 - acc: 0.7968\n","Epoch 191: val_loss did not improve from 0.15292\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1532 - acc: 0.7969 - val_loss: 0.1530 - val_acc: 0.7957\n","Epoch 192/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1530 - acc: 0.7966\n","Epoch 192: val_loss did not improve from 0.15292\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1532 - acc: 0.7966 - val_loss: 0.1531 - val_acc: 0.7963\n","Epoch 193/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1531 - acc: 0.7974\n","Epoch 193: val_loss improved from 0.15292 to 0.15215, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1530 - acc: 0.7973 - val_loss: 0.1522 - val_acc: 0.7993\n","Epoch 194/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.7972\n","Epoch 194: val_loss did not improve from 0.15215\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1528 - acc: 0.7972 - val_loss: 0.1529 - val_acc: 0.7981\n","Epoch 195/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1527 - acc: 0.7977\n","Epoch 195: val_loss did not improve from 0.15215\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1527 - acc: 0.7978 - val_loss: 0.1529 - val_acc: 0.7984\n","Epoch 196/1000\n","205/214 [===========================>..] - ETA: 0s - loss: 0.1528 - acc: 0.7980\n","Epoch 196: val_loss did not improve from 0.15215\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1527 - acc: 0.7979 - val_loss: 0.1528 - val_acc: 0.7977\n","Epoch 197/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1525 - acc: 0.7977\n","Epoch 197: val_loss did not improve from 0.15215\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1525 - acc: 0.7977 - val_loss: 0.1522 - val_acc: 0.7990\n","Epoch 198/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1523 - acc: 0.7984\n","Epoch 198: val_loss improved from 0.15215 to 0.15165, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1523 - acc: 0.7983 - val_loss: 0.1516 - val_acc: 0.7971\n","Epoch 199/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1523 - acc: 0.7986\n","Epoch 199: val_loss did not improve from 0.15165\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1523 - acc: 0.7986 - val_loss: 0.1538 - val_acc: 0.7943\n","Epoch 200/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1522 - acc: 0.7988\n","Epoch 200: val_loss did not improve from 0.15165\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1522 - acc: 0.7986 - val_loss: 0.1533 - val_acc: 0.7993\n","Epoch 201/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1520 - acc: 0.7993\n","Epoch 201: val_loss did not improve from 0.15165\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1520 - acc: 0.7991 - val_loss: 0.1519 - val_acc: 0.7973\n","Epoch 202/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1519 - acc: 0.7994\n","Epoch 202: val_loss did not improve from 0.15165\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1518 - acc: 0.7994 - val_loss: 0.1517 - val_acc: 0.7998\n","Epoch 203/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.7998\n","Epoch 203: val_loss improved from 0.15165 to 0.15112, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1517 - acc: 0.7998 - val_loss: 0.1511 - val_acc: 0.8004\n","Epoch 204/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1517 - acc: 0.7995\n","Epoch 204: val_loss did not improve from 0.15112\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1517 - acc: 0.7995 - val_loss: 0.1517 - val_acc: 0.7981\n","Epoch 205/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.8001\n","Epoch 205: val_loss did not improve from 0.15112\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1514 - acc: 0.7999 - val_loss: 0.1523 - val_acc: 0.8006\n","Epoch 206/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1513 - acc: 0.7996\n","Epoch 206: val_loss did not improve from 0.15112\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1513 - acc: 0.7996 - val_loss: 0.1514 - val_acc: 0.8006\n","Epoch 207/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1514 - acc: 0.8002\n","Epoch 207: val_loss improved from 0.15112 to 0.15109, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1513 - acc: 0.8003 - val_loss: 0.1511 - val_acc: 0.8002\n","Epoch 208/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1510 - acc: 0.8008\n","Epoch 208: val_loss did not improve from 0.15109\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1510 - acc: 0.8008 - val_loss: 0.1514 - val_acc: 0.7995\n","Epoch 209/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1511 - acc: 0.8005\n","Epoch 209: val_loss did not improve from 0.15109\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1511 - acc: 0.8003 - val_loss: 0.1534 - val_acc: 0.7988\n","Epoch 210/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1513 - acc: 0.7996\n","Epoch 210: val_loss improved from 0.15109 to 0.15087, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1512 - acc: 0.7998 - val_loss: 0.1509 - val_acc: 0.8008\n","Epoch 211/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1507 - acc: 0.8008\n","Epoch 211: val_loss did not improve from 0.15087\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1507 - acc: 0.8009 - val_loss: 0.1509 - val_acc: 0.8003\n","Epoch 212/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.8008\n","Epoch 212: val_loss did not improve from 0.15087\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1505 - acc: 0.8008 - val_loss: 0.1521 - val_acc: 0.8013\n","Epoch 213/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.8017\n","Epoch 213: val_loss did not improve from 0.15087\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1505 - acc: 0.8014 - val_loss: 0.1513 - val_acc: 0.8005\n","Epoch 214/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1505 - acc: 0.8016\n","Epoch 214: val_loss did not improve from 0.15087\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1505 - acc: 0.8015 - val_loss: 0.1524 - val_acc: 0.7976\n","Epoch 215/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1506 - acc: 0.8015\n","Epoch 215: val_loss did not improve from 0.15087\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1506 - acc: 0.8015 - val_loss: 0.1512 - val_acc: 0.7977\n","Epoch 216/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1502 - acc: 0.8012\n","Epoch 216: val_loss improved from 0.15087 to 0.15069, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1504 - acc: 0.8012 - val_loss: 0.1507 - val_acc: 0.8016\n","Epoch 217/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1502 - acc: 0.8015\n","Epoch 217: val_loss improved from 0.15069 to 0.15011, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1502 - acc: 0.8015 - val_loss: 0.1501 - val_acc: 0.8019\n","Epoch 218/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1500 - acc: 0.8018\n","Epoch 218: val_loss did not improve from 0.15011\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1500 - acc: 0.8018 - val_loss: 0.1520 - val_acc: 0.7979\n","Epoch 219/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1499 - acc: 0.8018\n","Epoch 219: val_loss improved from 0.15011 to 0.14964, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1499 - acc: 0.8020 - val_loss: 0.1496 - val_acc: 0.8025\n","Epoch 220/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.8023\n","Epoch 220: val_loss did not improve from 0.14964\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1498 - acc: 0.8023 - val_loss: 0.1503 - val_acc: 0.8031\n","Epoch 221/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1498 - acc: 0.8023\n","Epoch 221: val_loss did not improve from 0.14964\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1497 - acc: 0.8022 - val_loss: 0.1503 - val_acc: 0.8023\n","Epoch 222/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1497 - acc: 0.8022\n","Epoch 222: val_loss did not improve from 0.14964\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1497 - acc: 0.8021 - val_loss: 0.1503 - val_acc: 0.7989\n","Epoch 223/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1496 - acc: 0.8023\n","Epoch 223: val_loss improved from 0.14964 to 0.14912, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1496 - acc: 0.8022 - val_loss: 0.1491 - val_acc: 0.8017\n","Epoch 224/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1495 - acc: 0.8027\n","Epoch 224: val_loss did not improve from 0.14912\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1495 - acc: 0.8027 - val_loss: 0.1493 - val_acc: 0.8022\n","Epoch 225/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1494 - acc: 0.8027\n","Epoch 225: val_loss improved from 0.14912 to 0.14876, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1494 - acc: 0.8028 - val_loss: 0.1488 - val_acc: 0.8028\n","Epoch 226/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1492 - acc: 0.8029\n","Epoch 226: val_loss did not improve from 0.14876\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1492 - acc: 0.8028 - val_loss: 0.1488 - val_acc: 0.8014\n","Epoch 227/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1491 - acc: 0.8029\n","Epoch 227: val_loss did not improve from 0.14876\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1492 - acc: 0.8029 - val_loss: 0.1493 - val_acc: 0.8026\n","Epoch 228/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.8032\n","Epoch 228: val_loss did not improve from 0.14876\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1490 - acc: 0.8033 - val_loss: 0.1509 - val_acc: 0.8032\n","Epoch 229/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.8035\n","Epoch 229: val_loss improved from 0.14876 to 0.14851, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1491 - acc: 0.8034 - val_loss: 0.1485 - val_acc: 0.8028\n","Epoch 230/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1490 - acc: 0.8030\n","Epoch 230: val_loss did not improve from 0.14851\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1490 - acc: 0.8031 - val_loss: 0.1504 - val_acc: 0.8014\n","Epoch 231/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1487 - acc: 0.8037\n","Epoch 231: val_loss did not improve from 0.14851\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1488 - acc: 0.8036 - val_loss: 0.1490 - val_acc: 0.8020\n","Epoch 232/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1489 - acc: 0.8035\n","Epoch 232: val_loss did not improve from 0.14851\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1488 - acc: 0.8035 - val_loss: 0.1485 - val_acc: 0.8015\n","Epoch 233/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.8030\n","Epoch 233: val_loss did not improve from 0.14851\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1488 - acc: 0.8030 - val_loss: 0.1490 - val_acc: 0.8008\n","Epoch 234/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1485 - acc: 0.8036\n","Epoch 234: val_loss did not improve from 0.14851\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1485 - acc: 0.8036 - val_loss: 0.1494 - val_acc: 0.8022\n","Epoch 235/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.8035\n","Epoch 235: val_loss did not improve from 0.14851\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1486 - acc: 0.8035 - val_loss: 0.1506 - val_acc: 0.8046\n","Epoch 236/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1484 - acc: 0.8044\n","Epoch 236: val_loss improved from 0.14851 to 0.14774, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1485 - acc: 0.8044 - val_loss: 0.1477 - val_acc: 0.8049\n","Epoch 237/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1485 - acc: 0.8042\n","Epoch 237: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1484 - acc: 0.8042 - val_loss: 0.1486 - val_acc: 0.8035\n","Epoch 238/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.8045\n","Epoch 238: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1483 - acc: 0.8044 - val_loss: 0.1487 - val_acc: 0.8032\n","Epoch 239/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.8044\n","Epoch 239: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1482 - acc: 0.8044 - val_loss: 0.1485 - val_acc: 0.8048\n","Epoch 240/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1486 - acc: 0.8039\n","Epoch 240: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1484 - acc: 0.8042 - val_loss: 0.1481 - val_acc: 0.8041\n","Epoch 241/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.8047\n","Epoch 241: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1482 - acc: 0.8048 - val_loss: 0.1481 - val_acc: 0.8041\n","Epoch 242/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.8049\n","Epoch 242: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1481 - acc: 0.8049 - val_loss: 0.1480 - val_acc: 0.8048\n","Epoch 243/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.8046\n","Epoch 243: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1480 - acc: 0.8046 - val_loss: 0.1478 - val_acc: 0.8038\n","Epoch 244/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1481 - acc: 0.8050\n","Epoch 244: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1480 - acc: 0.8050 - val_loss: 0.1481 - val_acc: 0.8032\n","Epoch 245/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1482 - acc: 0.8053\n","Epoch 245: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1481 - acc: 0.8051 - val_loss: 0.1478 - val_acc: 0.8025\n","Epoch 246/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.8050\n","Epoch 246: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1479 - acc: 0.8050 - val_loss: 0.1480 - val_acc: 0.8043\n","Epoch 247/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1478 - acc: 0.8055\n","Epoch 247: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1478 - acc: 0.8053 - val_loss: 0.1485 - val_acc: 0.8032\n","Epoch 248/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1479 - acc: 0.8054\n","Epoch 248: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1478 - acc: 0.8055 - val_loss: 0.1484 - val_acc: 0.8008\n","Epoch 249/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.8053\n","Epoch 249: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1476 - acc: 0.8053 - val_loss: 0.1481 - val_acc: 0.8065\n","Epoch 250/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.8056\n","Epoch 250: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1476 - acc: 0.8056 - val_loss: 0.1487 - val_acc: 0.8031\n","Epoch 251/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.8053\n","Epoch 251: val_loss did not improve from 0.14774\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1476 - acc: 0.8053 - val_loss: 0.1484 - val_acc: 0.8039\n","Epoch 252/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1476 - acc: 0.8059\n","Epoch 252: val_loss improved from 0.14774 to 0.14749, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1476 - acc: 0.8060 - val_loss: 0.1475 - val_acc: 0.8067\n","Epoch 253/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.8063\n","Epoch 253: val_loss improved from 0.14749 to 0.14741, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1474 - acc: 0.8061 - val_loss: 0.1474 - val_acc: 0.8071\n","Epoch 254/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1474 - acc: 0.8057\n","Epoch 254: val_loss did not improve from 0.14741\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1474 - acc: 0.8056 - val_loss: 0.1480 - val_acc: 0.8044\n","Epoch 255/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.8065\n","Epoch 255: val_loss did not improve from 0.14741\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1473 - acc: 0.8064 - val_loss: 0.1477 - val_acc: 0.8049\n","Epoch 256/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.8065\n","Epoch 256: val_loss improved from 0.14741 to 0.14705, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 11ms/step - loss: 0.1473 - acc: 0.8065 - val_loss: 0.1470 - val_acc: 0.8055\n","Epoch 257/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.8061\n","Epoch 257: val_loss did not improve from 0.14705\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1473 - acc: 0.8061 - val_loss: 0.1479 - val_acc: 0.8065\n","Epoch 258/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1471 - acc: 0.8064\n","Epoch 258: val_loss did not improve from 0.14705\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1471 - acc: 0.8064 - val_loss: 0.1472 - val_acc: 0.8040\n","Epoch 259/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1473 - acc: 0.8057\n","Epoch 259: val_loss did not improve from 0.14705\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1472 - acc: 0.8058 - val_loss: 0.1478 - val_acc: 0.8065\n","Epoch 260/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1472 - acc: 0.8063\n","Epoch 260: val_loss did not improve from 0.14705\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1472 - acc: 0.8064 - val_loss: 0.1479 - val_acc: 0.8057\n","Epoch 261/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1471 - acc: 0.8058\n","Epoch 261: val_loss improved from 0.14705 to 0.14682, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1471 - acc: 0.8058 - val_loss: 0.1468 - val_acc: 0.8078\n","Epoch 262/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1469 - acc: 0.8071\n","Epoch 262: val_loss did not improve from 0.14682\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1469 - acc: 0.8071 - val_loss: 0.1473 - val_acc: 0.8068\n","Epoch 263/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1471 - acc: 0.8062\n","Epoch 263: val_loss did not improve from 0.14682\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1469 - acc: 0.8065 - val_loss: 0.1474 - val_acc: 0.8062\n","Epoch 264/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.8066\n","Epoch 264: val_loss did not improve from 0.14682\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1469 - acc: 0.8067 - val_loss: 0.1475 - val_acc: 0.8053\n","Epoch 265/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1470 - acc: 0.8067\n","Epoch 265: val_loss did not improve from 0.14682\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1470 - acc: 0.8067 - val_loss: 0.1479 - val_acc: 0.8016\n","Epoch 266/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1468 - acc: 0.8063\n","Epoch 266: val_loss did not improve from 0.14682\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1468 - acc: 0.8063 - val_loss: 0.1496 - val_acc: 0.8022\n","Epoch 267/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1467 - acc: 0.8069\n","Epoch 267: val_loss did not improve from 0.14682\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1467 - acc: 0.8069 - val_loss: 0.1480 - val_acc: 0.8030\n","Epoch 268/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.8071\n","Epoch 268: val_loss did not improve from 0.14682\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1467 - acc: 0.8070 - val_loss: 0.1481 - val_acc: 0.8065\n","Epoch 269/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.8068\n","Epoch 269: val_loss did not improve from 0.14682\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1466 - acc: 0.8068 - val_loss: 0.1471 - val_acc: 0.8071\n","Epoch 270/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.8074\n","Epoch 270: val_loss did not improve from 0.14682\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1465 - acc: 0.8072 - val_loss: 0.1472 - val_acc: 0.8063\n","Epoch 271/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1467 - acc: 0.8072\n","Epoch 271: val_loss improved from 0.14682 to 0.14668, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1466 - acc: 0.8071 - val_loss: 0.1467 - val_acc: 0.8058\n","Epoch 272/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1465 - acc: 0.8071\n","Epoch 272: val_loss did not improve from 0.14668\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1465 - acc: 0.8071 - val_loss: 0.1473 - val_acc: 0.8077\n","Epoch 273/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1465 - acc: 0.8072\n","Epoch 273: val_loss did not improve from 0.14668\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1465 - acc: 0.8072 - val_loss: 0.1478 - val_acc: 0.8032\n","Epoch 274/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1463 - acc: 0.8074\n","Epoch 274: val_loss did not improve from 0.14668\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1463 - acc: 0.8075 - val_loss: 0.1476 - val_acc: 0.8046\n","Epoch 275/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1465 - acc: 0.8078\n","Epoch 275: val_loss did not improve from 0.14668\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1465 - acc: 0.8078 - val_loss: 0.1474 - val_acc: 0.8083\n","Epoch 276/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1463 - acc: 0.8076\n","Epoch 276: val_loss did not improve from 0.14668\n","214/214 [==============================] - 1s 6ms/step - loss: 0.1463 - acc: 0.8076 - val_loss: 0.1467 - val_acc: 0.8041\n","Epoch 277/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.8075\n","Epoch 277: val_loss did not improve from 0.14668\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1462 - acc: 0.8075 - val_loss: 0.1471 - val_acc: 0.8052\n","Epoch 278/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1461 - acc: 0.8080\n","Epoch 278: val_loss improved from 0.14668 to 0.14632, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1460 - acc: 0.8079 - val_loss: 0.1463 - val_acc: 0.8049\n","Epoch 279/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1461 - acc: 0.8082\n","Epoch 279: val_loss did not improve from 0.14632\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1461 - acc: 0.8082 - val_loss: 0.1469 - val_acc: 0.8068\n","Epoch 280/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1462 - acc: 0.8081\n","Epoch 280: val_loss did not improve from 0.14632\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1462 - acc: 0.8081 - val_loss: 0.1466 - val_acc: 0.8054\n","Epoch 281/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1461 - acc: 0.8077\n","Epoch 281: val_loss did not improve from 0.14632\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1461 - acc: 0.8077 - val_loss: 0.1465 - val_acc: 0.8096\n","Epoch 282/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.8082\n","Epoch 282: val_loss improved from 0.14632 to 0.14606, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1459 - acc: 0.8080 - val_loss: 0.1461 - val_acc: 0.8078\n","Epoch 283/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1460 - acc: 0.8082\n","Epoch 283: val_loss did not improve from 0.14606\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1459 - acc: 0.8083 - val_loss: 0.1467 - val_acc: 0.8072\n","Epoch 284/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.8079\n","Epoch 284: val_loss improved from 0.14606 to 0.14581, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1458 - acc: 0.8079 - val_loss: 0.1458 - val_acc: 0.8091\n","Epoch 285/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1459 - acc: 0.8085\n","Epoch 285: val_loss did not improve from 0.14581\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1459 - acc: 0.8085 - val_loss: 0.1461 - val_acc: 0.8080\n","Epoch 286/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.8086\n","Epoch 286: val_loss did not improve from 0.14581\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1454 - acc: 0.8085 - val_loss: 0.1476 - val_acc: 0.8063\n","Epoch 287/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1457 - acc: 0.8087\n","Epoch 287: val_loss did not improve from 0.14581\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1457 - acc: 0.8087 - val_loss: 0.1467 - val_acc: 0.8056\n","Epoch 288/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.8085\n","Epoch 288: val_loss did not improve from 0.14581\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1455 - acc: 0.8085 - val_loss: 0.1460 - val_acc: 0.8088\n","Epoch 289/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.8089\n","Epoch 289: val_loss improved from 0.14581 to 0.14567, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1455 - acc: 0.8090 - val_loss: 0.1457 - val_acc: 0.8093\n","Epoch 290/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.8088\n","Epoch 290: val_loss did not improve from 0.14567\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1454 - acc: 0.8088 - val_loss: 0.1457 - val_acc: 0.8081\n","Epoch 291/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1455 - acc: 0.8091\n","Epoch 291: val_loss did not improve from 0.14567\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1455 - acc: 0.8091 - val_loss: 0.1457 - val_acc: 0.8091\n","Epoch 292/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1453 - acc: 0.8087\n","Epoch 292: val_loss did not improve from 0.14567\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1453 - acc: 0.8088 - val_loss: 0.1459 - val_acc: 0.8069\n","Epoch 293/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1454 - acc: 0.8084\n","Epoch 293: val_loss improved from 0.14567 to 0.14510, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1454 - acc: 0.8084 - val_loss: 0.1451 - val_acc: 0.8083\n","Epoch 294/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.8090\n","Epoch 294: val_loss did not improve from 0.14510\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1451 - acc: 0.8091 - val_loss: 0.1472 - val_acc: 0.8071\n","Epoch 295/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.8093\n","Epoch 295: val_loss did not improve from 0.14510\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1452 - acc: 0.8093 - val_loss: 0.1460 - val_acc: 0.8066\n","Epoch 296/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.8093\n","Epoch 296: val_loss did not improve from 0.14510\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1452 - acc: 0.8093 - val_loss: 0.1458 - val_acc: 0.8072\n","Epoch 297/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1452 - acc: 0.8092\n","Epoch 297: val_loss did not improve from 0.14510\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1452 - acc: 0.8091 - val_loss: 0.1454 - val_acc: 0.8093\n","Epoch 298/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1450 - acc: 0.8090\n","Epoch 298: val_loss did not improve from 0.14510\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1450 - acc: 0.8091 - val_loss: 0.1470 - val_acc: 0.8064\n","Epoch 299/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1451 - acc: 0.8091\n","Epoch 299: val_loss did not improve from 0.14510\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1451 - acc: 0.8091 - val_loss: 0.1468 - val_acc: 0.8065\n","Epoch 300/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1451 - acc: 0.8092\n","Epoch 300: val_loss did not improve from 0.14510\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1451 - acc: 0.8091 - val_loss: 0.1459 - val_acc: 0.8097\n","Epoch 301/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.8092\n","Epoch 301: val_loss improved from 0.14510 to 0.14470, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1449 - acc: 0.8092 - val_loss: 0.1447 - val_acc: 0.8093\n","Epoch 302/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.8094\n","Epoch 302: val_loss did not improve from 0.14470\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1449 - acc: 0.8095 - val_loss: 0.1461 - val_acc: 0.8070\n","Epoch 303/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.8098\n","Epoch 303: val_loss did not improve from 0.14470\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1446 - acc: 0.8099 - val_loss: 0.1460 - val_acc: 0.8074\n","Epoch 304/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.8096\n","Epoch 304: val_loss did not improve from 0.14470\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1447 - acc: 0.8096 - val_loss: 0.1458 - val_acc: 0.8074\n","Epoch 305/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.8101\n","Epoch 305: val_loss did not improve from 0.14470\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1447 - acc: 0.8099 - val_loss: 0.1448 - val_acc: 0.8099\n","Epoch 306/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1448 - acc: 0.8091\n","Epoch 306: val_loss did not improve from 0.14470\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1449 - acc: 0.8090 - val_loss: 0.1448 - val_acc: 0.8101\n","Epoch 307/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.8099\n","Epoch 307: val_loss did not improve from 0.14470\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1447 - acc: 0.8100 - val_loss: 0.1456 - val_acc: 0.8062\n","Epoch 308/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1446 - acc: 0.8094\n","Epoch 308: val_loss did not improve from 0.14470\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1447 - acc: 0.8093 - val_loss: 0.1459 - val_acc: 0.8086\n","Epoch 309/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.8101\n","Epoch 309: val_loss improved from 0.14470 to 0.14449, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1447 - acc: 0.8101 - val_loss: 0.1445 - val_acc: 0.8086\n","Epoch 310/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1449 - acc: 0.8099\n","Epoch 310: val_loss did not improve from 0.14449\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1449 - acc: 0.8099 - val_loss: 0.1461 - val_acc: 0.8039\n","Epoch 311/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1445 - acc: 0.8099\n","Epoch 311: val_loss did not improve from 0.14449\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1445 - acc: 0.8099 - val_loss: 0.1451 - val_acc: 0.8095\n","Epoch 312/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.8099\n","Epoch 312: val_loss did not improve from 0.14449\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1444 - acc: 0.8098 - val_loss: 0.1461 - val_acc: 0.8090\n","Epoch 313/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1447 - acc: 0.8096\n","Epoch 313: val_loss did not improve from 0.14449\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1447 - acc: 0.8096 - val_loss: 0.1449 - val_acc: 0.8101\n","Epoch 314/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1442 - acc: 0.8107\n","Epoch 314: val_loss did not improve from 0.14449\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1443 - acc: 0.8104 - val_loss: 0.1453 - val_acc: 0.8087\n","Epoch 315/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1444 - acc: 0.8096\n","Epoch 315: val_loss did not improve from 0.14449\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1444 - acc: 0.8097 - val_loss: 0.1448 - val_acc: 0.8103\n","Epoch 316/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.8103\n","Epoch 316: val_loss improved from 0.14449 to 0.14430, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1443 - acc: 0.8101 - val_loss: 0.1443 - val_acc: 0.8071\n","Epoch 317/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.8101\n","Epoch 317: val_loss improved from 0.14430 to 0.14418, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1442 - acc: 0.8101 - val_loss: 0.1442 - val_acc: 0.8116\n","Epoch 318/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.8102\n","Epoch 318: val_loss did not improve from 0.14418\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1442 - acc: 0.8103 - val_loss: 0.1447 - val_acc: 0.8071\n","Epoch 319/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1443 - acc: 0.8100\n","Epoch 319: val_loss did not improve from 0.14418\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1442 - acc: 0.8100 - val_loss: 0.1445 - val_acc: 0.8107\n","Epoch 320/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.8103\n","Epoch 320: val_loss did not improve from 0.14418\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1442 - acc: 0.8103 - val_loss: 0.1444 - val_acc: 0.8097\n","Epoch 321/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1442 - acc: 0.8103\n","Epoch 321: val_loss did not improve from 0.14418\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1442 - acc: 0.8103 - val_loss: 0.1448 - val_acc: 0.8061\n","Epoch 322/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.8102\n","Epoch 322: val_loss did not improve from 0.14418\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1441 - acc: 0.8100 - val_loss: 0.1446 - val_acc: 0.8083\n","Epoch 323/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.8098\n","Epoch 323: val_loss improved from 0.14418 to 0.14403, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1439 - acc: 0.8098 - val_loss: 0.1440 - val_acc: 0.8086\n","Epoch 324/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1440 - acc: 0.8097\n","Epoch 324: val_loss did not improve from 0.14403\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1440 - acc: 0.8097 - val_loss: 0.1447 - val_acc: 0.8071\n","Epoch 325/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.8106\n","Epoch 325: val_loss did not improve from 0.14403\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1438 - acc: 0.8104 - val_loss: 0.1450 - val_acc: 0.8089\n","Epoch 326/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1439 - acc: 0.8110\n","Epoch 326: val_loss did not improve from 0.14403\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1439 - acc: 0.8108 - val_loss: 0.1443 - val_acc: 0.8074\n","Epoch 327/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1438 - acc: 0.8102\n","Epoch 327: val_loss did not improve from 0.14403\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1438 - acc: 0.8102 - val_loss: 0.1446 - val_acc: 0.8086\n","Epoch 328/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.8100\n","Epoch 328: val_loss did not improve from 0.14403\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1439 - acc: 0.8101 - val_loss: 0.1444 - val_acc: 0.8101\n","Epoch 329/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1438 - acc: 0.8094\n","Epoch 329: val_loss did not improve from 0.14403\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1438 - acc: 0.8095 - val_loss: 0.1443 - val_acc: 0.8095\n","Epoch 330/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1437 - acc: 0.8099\n","Epoch 330: val_loss did not improve from 0.14403\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1436 - acc: 0.8100 - val_loss: 0.1460 - val_acc: 0.8102\n","Epoch 331/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1435 - acc: 0.8101\n","Epoch 331: val_loss did not improve from 0.14403\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1435 - acc: 0.8101 - val_loss: 0.1444 - val_acc: 0.8096\n","Epoch 332/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1433 - acc: 0.8105\n","Epoch 332: val_loss did not improve from 0.14403\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1433 - acc: 0.8105 - val_loss: 0.1446 - val_acc: 0.8093\n","Epoch 333/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1435 - acc: 0.8099\n","Epoch 333: val_loss improved from 0.14403 to 0.14380, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1434 - acc: 0.8101 - val_loss: 0.1438 - val_acc: 0.8097\n","Epoch 334/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.8099\n","Epoch 334: val_loss did not improve from 0.14380\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1433 - acc: 0.8099 - val_loss: 0.1443 - val_acc: 0.8090\n","Epoch 335/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1434 - acc: 0.8101\n","Epoch 335: val_loss did not improve from 0.14380\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1434 - acc: 0.8101 - val_loss: 0.1442 - val_acc: 0.8101\n","Epoch 336/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.8101\n","Epoch 336: val_loss did not improve from 0.14380\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1433 - acc: 0.8102 - val_loss: 0.1450 - val_acc: 0.8066\n","Epoch 337/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1432 - acc: 0.8103\n","Epoch 337: val_loss did not improve from 0.14380\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1432 - acc: 0.8103 - val_loss: 0.1457 - val_acc: 0.8059\n","Epoch 338/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.8103\n","Epoch 338: val_loss improved from 0.14380 to 0.14326, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1432 - acc: 0.8101 - val_loss: 0.1433 - val_acc: 0.8100\n","Epoch 339/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1433 - acc: 0.8101\n","Epoch 339: val_loss did not improve from 0.14326\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1433 - acc: 0.8101 - val_loss: 0.1441 - val_acc: 0.8093\n","Epoch 340/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.8101\n","Epoch 340: val_loss did not improve from 0.14326\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1431 - acc: 0.8104 - val_loss: 0.1476 - val_acc: 0.8046\n","Epoch 341/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.8099\n","Epoch 341: val_loss did not improve from 0.14326\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1432 - acc: 0.8099 - val_loss: 0.1443 - val_acc: 0.8080\n","Epoch 342/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1430 - acc: 0.8108\n","Epoch 342: val_loss did not improve from 0.14326\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1430 - acc: 0.8107 - val_loss: 0.1435 - val_acc: 0.8076\n","Epoch 343/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1429 - acc: 0.8100\n","Epoch 343: val_loss did not improve from 0.14326\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1429 - acc: 0.8100 - val_loss: 0.1434 - val_acc: 0.8109\n","Epoch 344/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1429 - acc: 0.8109\n","Epoch 344: val_loss did not improve from 0.14326\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1429 - acc: 0.8109 - val_loss: 0.1444 - val_acc: 0.8063\n","Epoch 345/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1430 - acc: 0.8103\n","Epoch 345: val_loss did not improve from 0.14326\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1430 - acc: 0.8103 - val_loss: 0.1436 - val_acc: 0.8077\n","Epoch 346/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1429 - acc: 0.8108\n","Epoch 346: val_loss improved from 0.14326 to 0.14318, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1429 - acc: 0.8106 - val_loss: 0.1432 - val_acc: 0.8095\n","Epoch 347/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.8108\n","Epoch 347: val_loss did not improve from 0.14318\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1427 - acc: 0.8108 - val_loss: 0.1435 - val_acc: 0.8074\n","Epoch 348/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.8106\n","Epoch 348: val_loss did not improve from 0.14318\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1428 - acc: 0.8106 - val_loss: 0.1436 - val_acc: 0.8075\n","Epoch 349/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1431 - acc: 0.8104\n","Epoch 349: val_loss did not improve from 0.14318\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1431 - acc: 0.8103 - val_loss: 0.1439 - val_acc: 0.8077\n","Epoch 350/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.8104\n","Epoch 350: val_loss did not improve from 0.14318\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1428 - acc: 0.8105 - val_loss: 0.1434 - val_acc: 0.8109\n","Epoch 351/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.8107\n","Epoch 351: val_loss did not improve from 0.14318\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1426 - acc: 0.8107 - val_loss: 0.1434 - val_acc: 0.8081\n","Epoch 352/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.8105\n","Epoch 352: val_loss improved from 0.14318 to 0.14294, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1426 - acc: 0.8105 - val_loss: 0.1429 - val_acc: 0.8071\n","Epoch 353/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1428 - acc: 0.8105\n","Epoch 353: val_loss did not improve from 0.14294\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1427 - acc: 0.8105 - val_loss: 0.1445 - val_acc: 0.8052\n","Epoch 354/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1427 - acc: 0.8103\n","Epoch 354: val_loss improved from 0.14294 to 0.14273, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1427 - acc: 0.8103 - val_loss: 0.1427 - val_acc: 0.8106\n","Epoch 355/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1426 - acc: 0.8106\n","Epoch 355: val_loss did not improve from 0.14273\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1426 - acc: 0.8106 - val_loss: 0.1432 - val_acc: 0.8080\n","Epoch 356/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.8102\n","Epoch 356: val_loss did not improve from 0.14273\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1426 - acc: 0.8100 - val_loss: 0.1430 - val_acc: 0.8089\n","Epoch 357/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1425 - acc: 0.8103\n","Epoch 357: val_loss did not improve from 0.14273\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1424 - acc: 0.8102 - val_loss: 0.1433 - val_acc: 0.8085\n","Epoch 358/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.8105\n","Epoch 358: val_loss did not improve from 0.14273\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1426 - acc: 0.8102 - val_loss: 0.1434 - val_acc: 0.8096\n","Epoch 359/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.8109\n","Epoch 359: val_loss did not improve from 0.14273\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1423 - acc: 0.8109 - val_loss: 0.1429 - val_acc: 0.8078\n","Epoch 360/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1426 - acc: 0.8106\n","Epoch 360: val_loss did not improve from 0.14273\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1425 - acc: 0.8106 - val_loss: 0.1432 - val_acc: 0.8109\n","Epoch 361/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.8105\n","Epoch 361: val_loss did not improve from 0.14273\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1424 - acc: 0.8106 - val_loss: 0.1429 - val_acc: 0.8082\n","Epoch 362/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1424 - acc: 0.8104\n","Epoch 362: val_loss did not improve from 0.14273\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1425 - acc: 0.8103 - val_loss: 0.1429 - val_acc: 0.8089\n","Epoch 363/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.8110\n","Epoch 363: val_loss did not improve from 0.14273\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1423 - acc: 0.8110 - val_loss: 0.1431 - val_acc: 0.8078\n","Epoch 364/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.8107\n","Epoch 364: val_loss did not improve from 0.14273\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1423 - acc: 0.8105 - val_loss: 0.1430 - val_acc: 0.8094\n","Epoch 365/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1422 - acc: 0.8107\n","Epoch 365: val_loss did not improve from 0.14273\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1422 - acc: 0.8107 - val_loss: 0.1434 - val_acc: 0.8101\n","Epoch 366/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1423 - acc: 0.8107\n","Epoch 366: val_loss improved from 0.14273 to 0.14263, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1423 - acc: 0.8107 - val_loss: 0.1426 - val_acc: 0.8096\n","Epoch 367/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1421 - acc: 0.8106\n","Epoch 367: val_loss improved from 0.14263 to 0.14248, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1421 - acc: 0.8106 - val_loss: 0.1425 - val_acc: 0.8106\n","Epoch 368/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.8099\n","Epoch 368: val_loss improved from 0.14248 to 0.14234, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1421 - acc: 0.8099 - val_loss: 0.1423 - val_acc: 0.8091\n","Epoch 369/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1420 - acc: 0.8109\n","Epoch 369: val_loss did not improve from 0.14234\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1420 - acc: 0.8109 - val_loss: 0.1425 - val_acc: 0.8101\n","Epoch 370/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1420 - acc: 0.8102\n","Epoch 370: val_loss improved from 0.14234 to 0.14199, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1421 - acc: 0.8101 - val_loss: 0.1420 - val_acc: 0.8104\n","Epoch 371/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1422 - acc: 0.8101\n","Epoch 371: val_loss did not improve from 0.14199\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1421 - acc: 0.8101 - val_loss: 0.1421 - val_acc: 0.8103\n","Epoch 372/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.8099\n","Epoch 372: val_loss improved from 0.14199 to 0.14173, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1418 - acc: 0.8098 - val_loss: 0.1417 - val_acc: 0.8099\n","Epoch 373/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.8101\n","Epoch 373: val_loss improved from 0.14173 to 0.14164, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1416 - acc: 0.8101 - val_loss: 0.1416 - val_acc: 0.8092\n","Epoch 374/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1416 - acc: 0.8101\n","Epoch 374: val_loss did not improve from 0.14164\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1416 - acc: 0.8100 - val_loss: 0.1419 - val_acc: 0.8105\n","Epoch 375/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1417 - acc: 0.8096\n","Epoch 375: val_loss did not improve from 0.14164\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1417 - acc: 0.8096 - val_loss: 0.1417 - val_acc: 0.8092\n","Epoch 376/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.8095\n","Epoch 376: val_loss did not improve from 0.14164\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1410 - acc: 0.8094 - val_loss: 0.1421 - val_acc: 0.8110\n","Epoch 377/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1410 - acc: 0.8098\n","Epoch 377: val_loss improved from 0.14164 to 0.14124, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1410 - acc: 0.8099 - val_loss: 0.1412 - val_acc: 0.8086\n","Epoch 378/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1408 - acc: 0.8095\n","Epoch 378: val_loss did not improve from 0.14124\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1408 - acc: 0.8095 - val_loss: 0.1434 - val_acc: 0.8101\n","Epoch 379/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.8094\n","Epoch 379: val_loss improved from 0.14124 to 0.14101, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1409 - acc: 0.8095 - val_loss: 0.1410 - val_acc: 0.8101\n","Epoch 380/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.8095\n","Epoch 380: val_loss did not improve from 0.14101\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1406 - acc: 0.8096 - val_loss: 0.1415 - val_acc: 0.8088\n","Epoch 381/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1408 - acc: 0.8097\n","Epoch 381: val_loss did not improve from 0.14101\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1407 - acc: 0.8097 - val_loss: 0.1415 - val_acc: 0.8086\n","Epoch 382/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1405 - acc: 0.8098\n","Epoch 382: val_loss did not improve from 0.14101\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1406 - acc: 0.8097 - val_loss: 0.1413 - val_acc: 0.8092\n","Epoch 383/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1406 - acc: 0.8089\n","Epoch 383: val_loss did not improve from 0.14101\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1406 - acc: 0.8090 - val_loss: 0.1417 - val_acc: 0.8056\n","Epoch 384/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.8094\n","Epoch 384: val_loss did not improve from 0.14101\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1404 - acc: 0.8095 - val_loss: 0.1414 - val_acc: 0.8094\n","Epoch 385/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1405 - acc: 0.8094\n","Epoch 385: val_loss improved from 0.14101 to 0.14100, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1405 - acc: 0.8094 - val_loss: 0.1410 - val_acc: 0.8070\n","Epoch 386/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.8095\n","Epoch 386: val_loss did not improve from 0.14100\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1404 - acc: 0.8095 - val_loss: 0.1416 - val_acc: 0.8081\n","Epoch 387/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1404 - acc: 0.8090\n","Epoch 387: val_loss improved from 0.14100 to 0.14070, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1404 - acc: 0.8090 - val_loss: 0.1407 - val_acc: 0.8087\n","Epoch 388/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.8097\n","Epoch 388: val_loss did not improve from 0.14070\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1402 - acc: 0.8094 - val_loss: 0.1413 - val_acc: 0.8077\n","Epoch 389/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1403 - acc: 0.8091\n","Epoch 389: val_loss did not improve from 0.14070\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1403 - acc: 0.8091 - val_loss: 0.1408 - val_acc: 0.8097\n","Epoch 390/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.8090\n","Epoch 390: val_loss did not improve from 0.14070\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1403 - acc: 0.8090 - val_loss: 0.1419 - val_acc: 0.8071\n","Epoch 391/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1402 - acc: 0.8091\n","Epoch 391: val_loss did not improve from 0.14070\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1401 - acc: 0.8092 - val_loss: 0.1413 - val_acc: 0.8076\n","Epoch 392/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1400 - acc: 0.8092\n","Epoch 392: val_loss improved from 0.14070 to 0.14067, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1400 - acc: 0.8091 - val_loss: 0.1407 - val_acc: 0.8080\n","Epoch 393/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.8090\n","Epoch 393: val_loss improved from 0.14067 to 0.14050, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1400 - acc: 0.8090 - val_loss: 0.1405 - val_acc: 0.8105\n","Epoch 394/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.8094\n","Epoch 394: val_loss did not improve from 0.14050\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1398 - acc: 0.8095 - val_loss: 0.1408 - val_acc: 0.8085\n","Epoch 395/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.8096\n","Epoch 395: val_loss did not improve from 0.14050\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1398 - acc: 0.8096 - val_loss: 0.1411 - val_acc: 0.8082\n","Epoch 396/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.8092\n","Epoch 396: val_loss did not improve from 0.14050\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1399 - acc: 0.8092 - val_loss: 0.1408 - val_acc: 0.8076\n","Epoch 397/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.8095\n","Epoch 397: val_loss did not improve from 0.14050\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1398 - acc: 0.8095 - val_loss: 0.1409 - val_acc: 0.8063\n","Epoch 398/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1400 - acc: 0.8092\n","Epoch 398: val_loss did not improve from 0.14050\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1400 - acc: 0.8092 - val_loss: 0.1413 - val_acc: 0.8091\n","Epoch 399/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.8088\n","Epoch 399: val_loss did not improve from 0.14050\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1398 - acc: 0.8088 - val_loss: 0.1409 - val_acc: 0.8060\n","Epoch 400/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.8089\n","Epoch 400: val_loss did not improve from 0.14050\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1397 - acc: 0.8090 - val_loss: 0.1409 - val_acc: 0.8070\n","Epoch 401/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1395 - acc: 0.8093\n","Epoch 401: val_loss did not improve from 0.14050\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1395 - acc: 0.8093 - val_loss: 0.1410 - val_acc: 0.8080\n","Epoch 402/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.8088\n","Epoch 402: val_loss did not improve from 0.14050\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1396 - acc: 0.8089 - val_loss: 0.1416 - val_acc: 0.8062\n","Epoch 403/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1397 - acc: 0.8087\n","Epoch 403: val_loss did not improve from 0.14050\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1397 - acc: 0.8086 - val_loss: 0.1409 - val_acc: 0.8070\n","Epoch 404/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.8090\n","Epoch 404: val_loss improved from 0.14050 to 0.14009, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1396 - acc: 0.8091 - val_loss: 0.1401 - val_acc: 0.8085\n","Epoch 405/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.8093\n","Epoch 405: val_loss did not improve from 0.14009\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1396 - acc: 0.8094 - val_loss: 0.1422 - val_acc: 0.8088\n","Epoch 406/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1399 - acc: 0.8087\n","Epoch 406: val_loss did not improve from 0.14009\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1399 - acc: 0.8088 - val_loss: 0.1405 - val_acc: 0.8075\n","Epoch 407/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.8093\n","Epoch 407: val_loss did not improve from 0.14009\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1395 - acc: 0.8091 - val_loss: 0.1411 - val_acc: 0.8077\n","Epoch 408/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1396 - acc: 0.8093\n","Epoch 408: val_loss did not improve from 0.14009\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1396 - acc: 0.8093 - val_loss: 0.1405 - val_acc: 0.8083\n","Epoch 409/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1395 - acc: 0.8095\n","Epoch 409: val_loss did not improve from 0.14009\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1395 - acc: 0.8095 - val_loss: 0.1404 - val_acc: 0.8057\n","Epoch 410/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1395 - acc: 0.8087\n","Epoch 410: val_loss did not improve from 0.14009\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1395 - acc: 0.8088 - val_loss: 0.1423 - val_acc: 0.8089\n","Epoch 411/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.8092\n","Epoch 411: val_loss improved from 0.14009 to 0.14002, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1393 - acc: 0.8092 - val_loss: 0.1400 - val_acc: 0.8086\n","Epoch 412/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1396 - acc: 0.8090\n","Epoch 412: val_loss did not improve from 0.14002\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1396 - acc: 0.8091 - val_loss: 0.1418 - val_acc: 0.8089\n","Epoch 413/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.8092\n","Epoch 413: val_loss did not improve from 0.14002\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1393 - acc: 0.8092 - val_loss: 0.1401 - val_acc: 0.8070\n","Epoch 414/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.8091\n","Epoch 414: val_loss did not improve from 0.14002\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1393 - acc: 0.8091 - val_loss: 0.1407 - val_acc: 0.8076\n","Epoch 415/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.8095\n","Epoch 415: val_loss improved from 0.14002 to 0.13972, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1394 - acc: 0.8095 - val_loss: 0.1397 - val_acc: 0.8082\n","Epoch 416/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1394 - acc: 0.8092\n","Epoch 416: val_loss improved from 0.13972 to 0.13963, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1394 - acc: 0.8092 - val_loss: 0.1396 - val_acc: 0.8078\n","Epoch 417/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.8095\n","Epoch 417: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1393 - acc: 0.8095 - val_loss: 0.1401 - val_acc: 0.8056\n","Epoch 418/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.8093\n","Epoch 418: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1393 - acc: 0.8093 - val_loss: 0.1407 - val_acc: 0.8055\n","Epoch 419/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.8085\n","Epoch 419: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 11ms/step - loss: 0.1397 - acc: 0.8086 - val_loss: 0.1405 - val_acc: 0.8060\n","Epoch 420/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.8088\n","Epoch 420: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1391 - acc: 0.8088 - val_loss: 0.1404 - val_acc: 0.8077\n","Epoch 421/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.8091\n","Epoch 421: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1393 - acc: 0.8091 - val_loss: 0.1406 - val_acc: 0.8083\n","Epoch 422/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.8096\n","Epoch 422: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1392 - acc: 0.8095 - val_loss: 0.1405 - val_acc: 0.8089\n","Epoch 423/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.8091\n","Epoch 423: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1391 - acc: 0.8090 - val_loss: 0.1403 - val_acc: 0.8082\n","Epoch 424/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.8094\n","Epoch 424: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1392 - acc: 0.8093 - val_loss: 0.1404 - val_acc: 0.8066\n","Epoch 425/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.8093\n","Epoch 425: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1392 - acc: 0.8093 - val_loss: 0.1408 - val_acc: 0.8091\n","Epoch 426/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.8095\n","Epoch 426: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1392 - acc: 0.8095 - val_loss: 0.1401 - val_acc: 0.8091\n","Epoch 427/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.8089\n","Epoch 427: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1391 - acc: 0.8088 - val_loss: 0.1407 - val_acc: 0.8055\n","Epoch 428/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.8088\n","Epoch 428: val_loss did not improve from 0.13963\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1391 - acc: 0.8088 - val_loss: 0.1408 - val_acc: 0.8082\n","Epoch 429/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.8088\n","Epoch 429: val_loss did not improve from 0.13963\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1392 - acc: 0.8088 - val_loss: 0.1401 - val_acc: 0.8086\n","Epoch 430/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1390 - acc: 0.8089\n","Epoch 430: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1390 - acc: 0.8089 - val_loss: 0.1408 - val_acc: 0.8077\n","Epoch 431/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.8091\n","Epoch 431: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1392 - acc: 0.8090 - val_loss: 0.1417 - val_acc: 0.8071\n","Epoch 432/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1393 - acc: 0.8091\n","Epoch 432: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1391 - acc: 0.8092 - val_loss: 0.1397 - val_acc: 0.8085\n","Epoch 433/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.8096\n","Epoch 433: val_loss did not improve from 0.13963\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1390 - acc: 0.8095 - val_loss: 0.1402 - val_acc: 0.8098\n","Epoch 434/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1392 - acc: 0.8085\n","Epoch 434: val_loss did not improve from 0.13963\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1390 - acc: 0.8087 - val_loss: 0.1398 - val_acc: 0.8100\n","Epoch 435/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.8091\n","Epoch 435: val_loss did not improve from 0.13963\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1391 - acc: 0.8090 - val_loss: 0.1406 - val_acc: 0.8089\n","Epoch 436/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.8090\n","Epoch 436: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1390 - acc: 0.8091 - val_loss: 0.1420 - val_acc: 0.8078\n","Epoch 437/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.8089\n","Epoch 437: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1390 - acc: 0.8089 - val_loss: 0.1404 - val_acc: 0.8093\n","Epoch 438/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.8090\n","Epoch 438: val_loss did not improve from 0.13963\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1390 - acc: 0.8088 - val_loss: 0.1406 - val_acc: 0.8071\n","Epoch 439/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1391 - acc: 0.8089\n","Epoch 439: val_loss improved from 0.13963 to 0.13946, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1391 - acc: 0.8088 - val_loss: 0.1395 - val_acc: 0.8070\n","Epoch 440/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.8091\n","Epoch 440: val_loss did not improve from 0.13946\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1389 - acc: 0.8091 - val_loss: 0.1399 - val_acc: 0.8095\n","Epoch 441/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.8092\n","Epoch 441: val_loss did not improve from 0.13946\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1388 - acc: 0.8092 - val_loss: 0.1399 - val_acc: 0.8073\n","Epoch 442/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.8090\n","Epoch 442: val_loss improved from 0.13946 to 0.13946, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1389 - acc: 0.8090 - val_loss: 0.1395 - val_acc: 0.8079\n","Epoch 443/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.8084\n","Epoch 443: val_loss improved from 0.13946 to 0.13940, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 11ms/step - loss: 0.1389 - acc: 0.8085 - val_loss: 0.1394 - val_acc: 0.8107\n","Epoch 444/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.8090\n","Epoch 444: val_loss did not improve from 0.13940\n","214/214 [==============================] - 3s 12ms/step - loss: 0.1389 - acc: 0.8091 - val_loss: 0.1408 - val_acc: 0.8089\n","Epoch 445/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.8091\n","Epoch 445: val_loss did not improve from 0.13940\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1389 - acc: 0.8091 - val_loss: 0.1399 - val_acc: 0.8071\n","Epoch 446/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.8083\n","Epoch 446: val_loss did not improve from 0.13940\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1388 - acc: 0.8085 - val_loss: 0.1408 - val_acc: 0.8072\n","Epoch 447/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.8089\n","Epoch 447: val_loss did not improve from 0.13940\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1389 - acc: 0.8089 - val_loss: 0.1398 - val_acc: 0.8076\n","Epoch 448/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1390 - acc: 0.8082\n","Epoch 448: val_loss did not improve from 0.13940\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1389 - acc: 0.8085 - val_loss: 0.1396 - val_acc: 0.8080\n","Epoch 449/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.8096\n","Epoch 449: val_loss did not improve from 0.13940\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1387 - acc: 0.8095 - val_loss: 0.1399 - val_acc: 0.8085\n","Epoch 450/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.8088\n","Epoch 450: val_loss improved from 0.13940 to 0.13935, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1389 - acc: 0.8087 - val_loss: 0.1393 - val_acc: 0.8079\n","Epoch 451/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.8087\n","Epoch 451: val_loss improved from 0.13935 to 0.13913, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1388 - acc: 0.8087 - val_loss: 0.1391 - val_acc: 0.8071\n","Epoch 452/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.8089\n","Epoch 452: val_loss did not improve from 0.13913\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1387 - acc: 0.8089 - val_loss: 0.1400 - val_acc: 0.8077\n","Epoch 453/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.8091\n","Epoch 453: val_loss did not improve from 0.13913\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1386 - acc: 0.8091 - val_loss: 0.1403 - val_acc: 0.8073\n","Epoch 454/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.8092\n","Epoch 454: val_loss did not improve from 0.13913\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1387 - acc: 0.8091 - val_loss: 0.1397 - val_acc: 0.8082\n","Epoch 455/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1386 - acc: 0.8092\n","Epoch 455: val_loss did not improve from 0.13913\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1386 - acc: 0.8092 - val_loss: 0.1397 - val_acc: 0.8095\n","Epoch 456/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.8088\n","Epoch 456: val_loss did not improve from 0.13913\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1387 - acc: 0.8088 - val_loss: 0.1406 - val_acc: 0.8064\n","Epoch 457/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1386 - acc: 0.8086\n","Epoch 457: val_loss did not improve from 0.13913\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1388 - acc: 0.8086 - val_loss: 0.1397 - val_acc: 0.8093\n","Epoch 458/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.8089\n","Epoch 458: val_loss improved from 0.13913 to 0.13895, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1387 - acc: 0.8090 - val_loss: 0.1390 - val_acc: 0.8070\n","Epoch 459/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.8086\n","Epoch 459: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1386 - acc: 0.8086 - val_loss: 0.1393 - val_acc: 0.8071\n","Epoch 460/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1386 - acc: 0.8087\n","Epoch 460: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1386 - acc: 0.8087 - val_loss: 0.1397 - val_acc: 0.8089\n","Epoch 461/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.8089\n","Epoch 461: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1388 - acc: 0.8090 - val_loss: 0.1393 - val_acc: 0.8097\n","Epoch 462/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1388 - acc: 0.8089\n","Epoch 462: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1388 - acc: 0.8089 - val_loss: 0.1396 - val_acc: 0.8086\n","Epoch 463/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1387 - acc: 0.8089\n","Epoch 463: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1387 - acc: 0.8089 - val_loss: 0.1397 - val_acc: 0.8074\n","Epoch 464/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.8094\n","Epoch 464: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1384 - acc: 0.8094 - val_loss: 0.1398 - val_acc: 0.8054\n","Epoch 465/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1389 - acc: 0.8088\n","Epoch 465: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1389 - acc: 0.8088 - val_loss: 0.1396 - val_acc: 0.8066\n","Epoch 466/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.8088\n","Epoch 466: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1385 - acc: 0.8088 - val_loss: 0.1400 - val_acc: 0.8072\n","Epoch 467/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.8091\n","Epoch 467: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1384 - acc: 0.8091 - val_loss: 0.1394 - val_acc: 0.8093\n","Epoch 468/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.8085\n","Epoch 468: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1384 - acc: 0.8085 - val_loss: 0.1401 - val_acc: 0.8081\n","Epoch 469/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1385 - acc: 0.8094\n","Epoch 469: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1385 - acc: 0.8094 - val_loss: 0.1400 - val_acc: 0.8050\n","Epoch 470/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.8088\n","Epoch 470: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1386 - acc: 0.8089 - val_loss: 0.1396 - val_acc: 0.8103\n","Epoch 471/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.8088\n","Epoch 471: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1385 - acc: 0.8088 - val_loss: 0.1400 - val_acc: 0.8078\n","Epoch 472/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.8094\n","Epoch 472: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1385 - acc: 0.8093 - val_loss: 0.1393 - val_acc: 0.8086\n","Epoch 473/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.8091\n","Epoch 473: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1386 - acc: 0.8090 - val_loss: 0.1396 - val_acc: 0.8053\n","Epoch 474/1000\n","206/214 [===========================>..] - ETA: 0s - loss: 0.1385 - acc: 0.8088\n","Epoch 474: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1386 - acc: 0.8090 - val_loss: 0.1406 - val_acc: 0.8080\n","Epoch 475/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.8095\n","Epoch 475: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1385 - acc: 0.8094 - val_loss: 0.1406 - val_acc: 0.8063\n","Epoch 476/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.8090\n","Epoch 476: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1385 - acc: 0.8090 - val_loss: 0.1391 - val_acc: 0.8097\n","Epoch 477/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.8091\n","Epoch 477: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1384 - acc: 0.8091 - val_loss: 0.1392 - val_acc: 0.8112\n","Epoch 478/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.8089\n","Epoch 478: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1384 - acc: 0.8088 - val_loss: 0.1392 - val_acc: 0.8088\n","Epoch 479/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.8086\n","Epoch 479: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1384 - acc: 0.8087 - val_loss: 0.1390 - val_acc: 0.8082\n","Epoch 480/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1383 - acc: 0.8098\n","Epoch 480: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1383 - acc: 0.8098 - val_loss: 0.1394 - val_acc: 0.8096\n","Epoch 481/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.8089\n","Epoch 481: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1383 - acc: 0.8089 - val_loss: 0.1409 - val_acc: 0.8064\n","Epoch 482/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.8088\n","Epoch 482: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1385 - acc: 0.8087 - val_loss: 0.1392 - val_acc: 0.8087\n","Epoch 483/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.8094\n","Epoch 483: val_loss did not improve from 0.13895\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1384 - acc: 0.8094 - val_loss: 0.1404 - val_acc: 0.8067\n","Epoch 484/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.8090\n","Epoch 484: val_loss improved from 0.13895 to 0.13894, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1384 - acc: 0.8089 - val_loss: 0.1389 - val_acc: 0.8100\n","Epoch 485/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.8091\n","Epoch 485: val_loss improved from 0.13894 to 0.13889, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1383 - acc: 0.8089 - val_loss: 0.1389 - val_acc: 0.8100\n","Epoch 486/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.8094\n","Epoch 486: val_loss improved from 0.13889 to 0.13884, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1383 - acc: 0.8091 - val_loss: 0.1388 - val_acc: 0.8066\n","Epoch 487/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1382 - acc: 0.8094\n","Epoch 487: val_loss did not improve from 0.13884\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1382 - acc: 0.8094 - val_loss: 0.1402 - val_acc: 0.8032\n","Epoch 488/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1385 - acc: 0.8091\n","Epoch 488: val_loss did not improve from 0.13884\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1385 - acc: 0.8090 - val_loss: 0.1396 - val_acc: 0.8066\n","Epoch 489/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.8089\n","Epoch 489: val_loss did not improve from 0.13884\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1382 - acc: 0.8090 - val_loss: 0.1391 - val_acc: 0.8075\n","Epoch 490/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.8093\n","Epoch 490: val_loss did not improve from 0.13884\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1382 - acc: 0.8095 - val_loss: 0.1400 - val_acc: 0.8092\n","Epoch 491/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.8092\n","Epoch 491: val_loss did not improve from 0.13884\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1384 - acc: 0.8091 - val_loss: 0.1394 - val_acc: 0.8070\n","Epoch 492/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1384 - acc: 0.8091\n","Epoch 492: val_loss improved from 0.13884 to 0.13868, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1384 - acc: 0.8091 - val_loss: 0.1387 - val_acc: 0.8095\n","Epoch 493/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.8093\n","Epoch 493: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1383 - acc: 0.8094 - val_loss: 0.1397 - val_acc: 0.8081\n","Epoch 494/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.8094\n","Epoch 494: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1381 - acc: 0.8094 - val_loss: 0.1398 - val_acc: 0.8108\n","Epoch 495/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1384 - acc: 0.8090\n","Epoch 495: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1384 - acc: 0.8090 - val_loss: 0.1401 - val_acc: 0.8047\n","Epoch 496/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.8091\n","Epoch 496: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1381 - acc: 0.8090 - val_loss: 0.1387 - val_acc: 0.8094\n","Epoch 497/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1380 - acc: 0.8095\n","Epoch 497: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1380 - acc: 0.8095 - val_loss: 0.1391 - val_acc: 0.8070\n","Epoch 498/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.8094\n","Epoch 498: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1382 - acc: 0.8094 - val_loss: 0.1392 - val_acc: 0.8071\n","Epoch 499/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.8092\n","Epoch 499: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1382 - acc: 0.8092 - val_loss: 0.1395 - val_acc: 0.8088\n","Epoch 500/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1383 - acc: 0.8093\n","Epoch 500: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1383 - acc: 0.8093 - val_loss: 0.1402 - val_acc: 0.8094\n","Epoch 501/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.8095\n","Epoch 501: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1382 - acc: 0.8095 - val_loss: 0.1397 - val_acc: 0.8041\n","Epoch 502/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1383 - acc: 0.8098\n","Epoch 502: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1382 - acc: 0.8096 - val_loss: 0.1405 - val_acc: 0.8059\n","Epoch 503/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.8096\n","Epoch 503: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1382 - acc: 0.8096 - val_loss: 0.1400 - val_acc: 0.8030\n","Epoch 504/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.8100\n","Epoch 504: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1380 - acc: 0.8100 - val_loss: 0.1389 - val_acc: 0.8119\n","Epoch 505/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.8100\n","Epoch 505: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1381 - acc: 0.8099 - val_loss: 0.1396 - val_acc: 0.8098\n","Epoch 506/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.8098\n","Epoch 506: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1381 - acc: 0.8098 - val_loss: 0.1388 - val_acc: 0.8095\n","Epoch 507/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.8102\n","Epoch 507: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1381 - acc: 0.8103 - val_loss: 0.1388 - val_acc: 0.8085\n","Epoch 508/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1381 - acc: 0.8100\n","Epoch 508: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1381 - acc: 0.8100 - val_loss: 0.1403 - val_acc: 0.8078\n","Epoch 509/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.8102\n","Epoch 509: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1379 - acc: 0.8101 - val_loss: 0.1392 - val_acc: 0.8100\n","Epoch 510/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8110\n","Epoch 510: val_loss did not improve from 0.13868\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1379 - acc: 0.8108 - val_loss: 0.1388 - val_acc: 0.8089\n","Epoch 511/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1380 - acc: 0.8107\n","Epoch 511: val_loss improved from 0.13868 to 0.13859, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1379 - acc: 0.8108 - val_loss: 0.1386 - val_acc: 0.8081\n","Epoch 512/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1380 - acc: 0.8105\n","Epoch 512: val_loss did not improve from 0.13859\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1380 - acc: 0.8105 - val_loss: 0.1391 - val_acc: 0.8080\n","Epoch 513/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.8107\n","Epoch 513: val_loss did not improve from 0.13859\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1379 - acc: 0.8107 - val_loss: 0.1394 - val_acc: 0.8074\n","Epoch 514/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1382 - acc: 0.8105\n","Epoch 514: val_loss did not improve from 0.13859\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1382 - acc: 0.8104 - val_loss: 0.1403 - val_acc: 0.8114\n","Epoch 515/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8108\n","Epoch 515: val_loss did not improve from 0.13859\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1378 - acc: 0.8109 - val_loss: 0.1397 - val_acc: 0.8089\n","Epoch 516/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8109\n","Epoch 516: val_loss did not improve from 0.13859\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1378 - acc: 0.8109 - val_loss: 0.1392 - val_acc: 0.8088\n","Epoch 517/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8111\n","Epoch 517: val_loss did not improve from 0.13859\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1379 - acc: 0.8109 - val_loss: 0.1392 - val_acc: 0.8104\n","Epoch 518/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8103\n","Epoch 518: val_loss did not improve from 0.13859\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1378 - acc: 0.8104 - val_loss: 0.1396 - val_acc: 0.8109\n","Epoch 519/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.8110\n","Epoch 519: val_loss improved from 0.13859 to 0.13828, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1378 - acc: 0.8108 - val_loss: 0.1383 - val_acc: 0.8102\n","Epoch 520/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.8111\n","Epoch 520: val_loss did not improve from 0.13828\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1377 - acc: 0.8110 - val_loss: 0.1388 - val_acc: 0.8082\n","Epoch 521/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8107\n","Epoch 521: val_loss improved from 0.13828 to 0.13826, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1378 - acc: 0.8108 - val_loss: 0.1383 - val_acc: 0.8092\n","Epoch 522/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.8107\n","Epoch 522: val_loss did not improve from 0.13826\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1376 - acc: 0.8109 - val_loss: 0.1386 - val_acc: 0.8095\n","Epoch 523/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8108\n","Epoch 523: val_loss did not improve from 0.13826\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1378 - acc: 0.8108 - val_loss: 0.1390 - val_acc: 0.8076\n","Epoch 524/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1379 - acc: 0.8105\n","Epoch 524: val_loss did not improve from 0.13826\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1379 - acc: 0.8105 - val_loss: 0.1384 - val_acc: 0.8079\n","Epoch 525/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.8110\n","Epoch 525: val_loss did not improve from 0.13826\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1377 - acc: 0.8110 - val_loss: 0.1385 - val_acc: 0.8103\n","Epoch 526/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8103\n","Epoch 526: val_loss improved from 0.13826 to 0.13821, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1378 - acc: 0.8103 - val_loss: 0.1382 - val_acc: 0.8110\n","Epoch 527/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.8109\n","Epoch 527: val_loss did not improve from 0.13821\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1376 - acc: 0.8109 - val_loss: 0.1385 - val_acc: 0.8098\n","Epoch 528/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.8109\n","Epoch 528: val_loss did not improve from 0.13821\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1375 - acc: 0.8107 - val_loss: 0.1388 - val_acc: 0.8076\n","Epoch 529/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.8106\n","Epoch 529: val_loss did not improve from 0.13821\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1377 - acc: 0.8106 - val_loss: 0.1391 - val_acc: 0.8095\n","Epoch 530/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.8107\n","Epoch 530: val_loss did not improve from 0.13821\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1376 - acc: 0.8107 - val_loss: 0.1385 - val_acc: 0.8096\n","Epoch 531/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8104\n","Epoch 531: val_loss did not improve from 0.13821\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1377 - acc: 0.8104 - val_loss: 0.1386 - val_acc: 0.8106\n","Epoch 532/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.8108\n","Epoch 532: val_loss did not improve from 0.13821\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1375 - acc: 0.8107 - val_loss: 0.1386 - val_acc: 0.8113\n","Epoch 533/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.8104\n","Epoch 533: val_loss did not improve from 0.13821\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1377 - acc: 0.8102 - val_loss: 0.1383 - val_acc: 0.8086\n","Epoch 534/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.8105\n","Epoch 534: val_loss did not improve from 0.13821\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1375 - acc: 0.8103 - val_loss: 0.1397 - val_acc: 0.8077\n","Epoch 535/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1378 - acc: 0.8102\n","Epoch 535: val_loss did not improve from 0.13821\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1378 - acc: 0.8101 - val_loss: 0.1386 - val_acc: 0.8093\n","Epoch 536/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.8107\n","Epoch 536: val_loss improved from 0.13821 to 0.13784, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1375 - acc: 0.8108 - val_loss: 0.1378 - val_acc: 0.8091\n","Epoch 537/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1375 - acc: 0.8108\n","Epoch 537: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1375 - acc: 0.8108 - val_loss: 0.1389 - val_acc: 0.8092\n","Epoch 538/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.8105\n","Epoch 538: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1376 - acc: 0.8104 - val_loss: 0.1380 - val_acc: 0.8100\n","Epoch 539/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.8104\n","Epoch 539: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1373 - acc: 0.8105 - val_loss: 0.1384 - val_acc: 0.8076\n","Epoch 540/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.8103\n","Epoch 540: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1374 - acc: 0.8104 - val_loss: 0.1385 - val_acc: 0.8113\n","Epoch 541/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.8104\n","Epoch 541: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1375 - acc: 0.8104 - val_loss: 0.1403 - val_acc: 0.8090\n","Epoch 542/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.8101\n","Epoch 542: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1376 - acc: 0.8100 - val_loss: 0.1382 - val_acc: 0.8100\n","Epoch 543/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.8103\n","Epoch 543: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1375 - acc: 0.8102 - val_loss: 0.1388 - val_acc: 0.8082\n","Epoch 544/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.8102\n","Epoch 544: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1376 - acc: 0.8102 - val_loss: 0.1387 - val_acc: 0.8100\n","Epoch 545/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.8101\n","Epoch 545: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1374 - acc: 0.8102 - val_loss: 0.1379 - val_acc: 0.8108\n","Epoch 546/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.8102\n","Epoch 546: val_loss did not improve from 0.13784\n","214/214 [==============================] - 1s 7ms/step - loss: 0.1374 - acc: 0.8102 - val_loss: 0.1383 - val_acc: 0.8084\n","Epoch 547/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.8097\n","Epoch 547: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1373 - acc: 0.8099 - val_loss: 0.1386 - val_acc: 0.8074\n","Epoch 548/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.8101\n","Epoch 548: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1375 - acc: 0.8101 - val_loss: 0.1383 - val_acc: 0.8111\n","Epoch 549/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.8099\n","Epoch 549: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1375 - acc: 0.8099 - val_loss: 0.1379 - val_acc: 0.8105\n","Epoch 550/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.8106\n","Epoch 550: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1372 - acc: 0.8106 - val_loss: 0.1382 - val_acc: 0.8075\n","Epoch 551/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.8098\n","Epoch 551: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1374 - acc: 0.8097 - val_loss: 0.1390 - val_acc: 0.8070\n","Epoch 552/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1375 - acc: 0.8098\n","Epoch 552: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1375 - acc: 0.8098 - val_loss: 0.1387 - val_acc: 0.8068\n","Epoch 553/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.8102\n","Epoch 553: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1372 - acc: 0.8103 - val_loss: 0.1385 - val_acc: 0.8082\n","Epoch 554/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.8102\n","Epoch 554: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1373 - acc: 0.8101 - val_loss: 0.1380 - val_acc: 0.8099\n","Epoch 555/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.8098\n","Epoch 555: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1373 - acc: 0.8098 - val_loss: 0.1394 - val_acc: 0.8098\n","Epoch 556/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.8103\n","Epoch 556: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1372 - acc: 0.8102 - val_loss: 0.1381 - val_acc: 0.8094\n","Epoch 557/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.8100\n","Epoch 557: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1374 - acc: 0.8101 - val_loss: 0.1388 - val_acc: 0.8092\n","Epoch 558/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1374 - acc: 0.8097\n","Epoch 558: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1374 - acc: 0.8097 - val_loss: 0.1472 - val_acc: 0.8027\n","Epoch 559/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1377 - acc: 0.8101\n","Epoch 559: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1377 - acc: 0.8100 - val_loss: 0.1382 - val_acc: 0.8108\n","Epoch 560/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.8099\n","Epoch 560: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1373 - acc: 0.8099 - val_loss: 0.1385 - val_acc: 0.8062\n","Epoch 561/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1372 - acc: 0.8100\n","Epoch 561: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1372 - acc: 0.8100 - val_loss: 0.1398 - val_acc: 0.8078\n","Epoch 562/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.8098\n","Epoch 562: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1373 - acc: 0.8099 - val_loss: 0.1379 - val_acc: 0.8104\n","Epoch 563/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8099\n","Epoch 563: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1370 - acc: 0.8099 - val_loss: 0.1392 - val_acc: 0.8068\n","Epoch 564/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.8101\n","Epoch 564: val_loss did not improve from 0.13784\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1372 - acc: 0.8102 - val_loss: 0.1384 - val_acc: 0.8101\n","Epoch 565/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1376 - acc: 0.8096\n","Epoch 565: val_loss improved from 0.13784 to 0.13752, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1375 - acc: 0.8097 - val_loss: 0.1375 - val_acc: 0.8110\n","Epoch 566/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8103\n","Epoch 566: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8102 - val_loss: 0.1380 - val_acc: 0.8101\n","Epoch 567/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8103\n","Epoch 567: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8103 - val_loss: 0.1392 - val_acc: 0.8091\n","Epoch 568/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.8096\n","Epoch 568: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1373 - acc: 0.8096 - val_loss: 0.1379 - val_acc: 0.8083\n","Epoch 569/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.8102\n","Epoch 569: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1372 - acc: 0.8101 - val_loss: 0.1380 - val_acc: 0.8100\n","Epoch 570/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1372 - acc: 0.8096\n","Epoch 570: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1372 - acc: 0.8096 - val_loss: 0.1377 - val_acc: 0.8091\n","Epoch 571/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8101\n","Epoch 571: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1370 - acc: 0.8101 - val_loss: 0.1388 - val_acc: 0.8095\n","Epoch 572/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8101\n","Epoch 572: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1371 - acc: 0.8102 - val_loss: 0.1391 - val_acc: 0.8080\n","Epoch 573/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8100\n","Epoch 573: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8099 - val_loss: 0.1383 - val_acc: 0.8091\n","Epoch 574/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1372 - acc: 0.8095\n","Epoch 574: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1372 - acc: 0.8095 - val_loss: 0.1385 - val_acc: 0.8085\n","Epoch 575/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8100\n","Epoch 575: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8100 - val_loss: 0.1386 - val_acc: 0.8082\n","Epoch 576/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1374 - acc: 0.8096\n","Epoch 576: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1373 - acc: 0.8095 - val_loss: 0.1379 - val_acc: 0.8108\n","Epoch 577/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1373 - acc: 0.8099\n","Epoch 577: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1373 - acc: 0.8100 - val_loss: 0.1378 - val_acc: 0.8091\n","Epoch 578/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8097\n","Epoch 578: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8097 - val_loss: 0.1376 - val_acc: 0.8090\n","Epoch 579/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8096\n","Epoch 579: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8097 - val_loss: 0.1378 - val_acc: 0.8112\n","Epoch 580/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1375 - acc: 0.8094\n","Epoch 580: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1373 - acc: 0.8093 - val_loss: 0.1388 - val_acc: 0.8084\n","Epoch 581/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8103\n","Epoch 581: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1370 - acc: 0.8102 - val_loss: 0.1377 - val_acc: 0.8098\n","Epoch 582/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8102\n","Epoch 582: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1369 - acc: 0.8101 - val_loss: 0.1384 - val_acc: 0.8089\n","Epoch 583/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8097\n","Epoch 583: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8097 - val_loss: 0.1381 - val_acc: 0.8085\n","Epoch 584/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8097\n","Epoch 584: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1369 - acc: 0.8097 - val_loss: 0.1384 - val_acc: 0.8093\n","Epoch 585/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8095\n","Epoch 585: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1370 - acc: 0.8096 - val_loss: 0.1387 - val_acc: 0.8089\n","Epoch 586/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8099\n","Epoch 586: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1370 - acc: 0.8100 - val_loss: 0.1379 - val_acc: 0.8087\n","Epoch 587/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8102\n","Epoch 587: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8102 - val_loss: 0.1389 - val_acc: 0.8078\n","Epoch 588/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8102\n","Epoch 588: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1370 - acc: 0.8100 - val_loss: 0.1380 - val_acc: 0.8110\n","Epoch 589/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8099\n","Epoch 589: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1369 - acc: 0.8101 - val_loss: 0.1380 - val_acc: 0.8093\n","Epoch 590/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8097\n","Epoch 590: val_loss did not improve from 0.13752\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8096 - val_loss: 0.1376 - val_acc: 0.8089\n","Epoch 591/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8103\n","Epoch 591: val_loss improved from 0.13752 to 0.13750, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1369 - acc: 0.8102 - val_loss: 0.1375 - val_acc: 0.8101\n","Epoch 592/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8102\n","Epoch 592: val_loss improved from 0.13750 to 0.13736, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1369 - acc: 0.8102 - val_loss: 0.1374 - val_acc: 0.8095\n","Epoch 593/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8097\n","Epoch 593: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1370 - acc: 0.8099 - val_loss: 0.1377 - val_acc: 0.8063\n","Epoch 594/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8100\n","Epoch 594: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1370 - acc: 0.8100 - val_loss: 0.1379 - val_acc: 0.8108\n","Epoch 595/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8104\n","Epoch 595: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1369 - acc: 0.8104 - val_loss: 0.1377 - val_acc: 0.8097\n","Epoch 596/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8098\n","Epoch 596: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1369 - acc: 0.8099 - val_loss: 0.1390 - val_acc: 0.8092\n","Epoch 597/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8102\n","Epoch 597: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1369 - acc: 0.8102 - val_loss: 0.1374 - val_acc: 0.8094\n","Epoch 598/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.8100\n","Epoch 598: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1368 - acc: 0.8099 - val_loss: 0.1381 - val_acc: 0.8078\n","Epoch 599/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1371 - acc: 0.8097\n","Epoch 599: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1371 - acc: 0.8098 - val_loss: 0.1379 - val_acc: 0.8092\n","Epoch 600/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8099\n","Epoch 600: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1369 - acc: 0.8101 - val_loss: 0.1377 - val_acc: 0.8089\n","Epoch 601/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.8103\n","Epoch 601: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1368 - acc: 0.8101 - val_loss: 0.1376 - val_acc: 0.8078\n","Epoch 602/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1368 - acc: 0.8105\n","Epoch 602: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1368 - acc: 0.8105 - val_loss: 0.1375 - val_acc: 0.8097\n","Epoch 603/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8102\n","Epoch 603: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1369 - acc: 0.8101 - val_loss: 0.1381 - val_acc: 0.8087\n","Epoch 604/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1368 - acc: 0.8100\n","Epoch 604: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1368 - acc: 0.8100 - val_loss: 0.1375 - val_acc: 0.8117\n","Epoch 605/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8101\n","Epoch 605: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1370 - acc: 0.8101 - val_loss: 0.1388 - val_acc: 0.8122\n","Epoch 606/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1369 - acc: 0.8100\n","Epoch 606: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1368 - acc: 0.8100 - val_loss: 0.1380 - val_acc: 0.8082\n","Epoch 607/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.8102\n","Epoch 607: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1368 - acc: 0.8101 - val_loss: 0.1377 - val_acc: 0.8077\n","Epoch 608/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.8099\n","Epoch 608: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1368 - acc: 0.8099 - val_loss: 0.1390 - val_acc: 0.8101\n","Epoch 609/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1370 - acc: 0.8100\n","Epoch 609: val_loss did not improve from 0.13736\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1369 - acc: 0.8101 - val_loss: 0.1390 - val_acc: 0.8083\n","Epoch 610/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.8109\n","Epoch 610: val_loss improved from 0.13736 to 0.13725, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1367 - acc: 0.8106 - val_loss: 0.1372 - val_acc: 0.8100\n","Epoch 611/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.8101\n","Epoch 611: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1367 - acc: 0.8100 - val_loss: 0.1375 - val_acc: 0.8078\n","Epoch 612/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.8101\n","Epoch 612: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1366 - acc: 0.8100 - val_loss: 0.1384 - val_acc: 0.8067\n","Epoch 613/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1367 - acc: 0.8101\n","Epoch 613: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1367 - acc: 0.8101 - val_loss: 0.1374 - val_acc: 0.8092\n","Epoch 614/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.8102\n","Epoch 614: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1367 - acc: 0.8102 - val_loss: 0.1381 - val_acc: 0.8099\n","Epoch 615/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1365 - acc: 0.8103\n","Epoch 615: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1365 - acc: 0.8103 - val_loss: 0.1373 - val_acc: 0.8117\n","Epoch 616/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.8105\n","Epoch 616: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1365 - acc: 0.8105 - val_loss: 0.1378 - val_acc: 0.8109\n","Epoch 617/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1366 - acc: 0.8103\n","Epoch 617: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1366 - acc: 0.8102 - val_loss: 0.1373 - val_acc: 0.8103\n","Epoch 618/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.8103\n","Epoch 618: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1365 - acc: 0.8103 - val_loss: 0.1383 - val_acc: 0.8116\n","Epoch 619/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.8105\n","Epoch 619: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1365 - acc: 0.8105 - val_loss: 0.1373 - val_acc: 0.8069\n","Epoch 620/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1368 - acc: 0.8102\n","Epoch 620: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1368 - acc: 0.8102 - val_loss: 0.1384 - val_acc: 0.8045\n","Epoch 621/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1365 - acc: 0.8107\n","Epoch 621: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1365 - acc: 0.8107 - val_loss: 0.1387 - val_acc: 0.8108\n","Epoch 622/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.8105\n","Epoch 622: val_loss did not improve from 0.13725\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1366 - acc: 0.8104 - val_loss: 0.1372 - val_acc: 0.8099\n","Epoch 623/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.8104\n","Epoch 623: val_loss improved from 0.13725 to 0.13693, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1363 - acc: 0.8104 - val_loss: 0.1369 - val_acc: 0.8098\n","Epoch 624/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1367 - acc: 0.8105\n","Epoch 624: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1366 - acc: 0.8104 - val_loss: 0.1375 - val_acc: 0.8094\n","Epoch 625/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.8103\n","Epoch 625: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1363 - acc: 0.8102 - val_loss: 0.1377 - val_acc: 0.8106\n","Epoch 626/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.8101\n","Epoch 626: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1363 - acc: 0.8101 - val_loss: 0.1376 - val_acc: 0.8113\n","Epoch 627/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.8102\n","Epoch 627: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1362 - acc: 0.8101 - val_loss: 0.1374 - val_acc: 0.8109\n","Epoch 628/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.8099\n","Epoch 628: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1363 - acc: 0.8100 - val_loss: 0.1399 - val_acc: 0.8019\n","Epoch 629/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.8095\n","Epoch 629: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 7ms/step - loss: 0.1365 - acc: 0.8095 - val_loss: 0.1382 - val_acc: 0.8088\n","Epoch 630/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1364 - acc: 0.8104\n","Epoch 630: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1364 - acc: 0.8104 - val_loss: 0.1375 - val_acc: 0.8081\n","Epoch 631/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1365 - acc: 0.8103\n","Epoch 631: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1364 - acc: 0.8102 - val_loss: 0.1379 - val_acc: 0.8075\n","Epoch 632/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.8102\n","Epoch 632: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1363 - acc: 0.8101 - val_loss: 0.1374 - val_acc: 0.8086\n","Epoch 633/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.8107\n","Epoch 633: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8108 - val_loss: 0.1378 - val_acc: 0.8072\n","Epoch 634/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.8101\n","Epoch 634: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1362 - acc: 0.8102 - val_loss: 0.1374 - val_acc: 0.8092\n","Epoch 635/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.8104\n","Epoch 635: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1363 - acc: 0.8103 - val_loss: 0.1371 - val_acc: 0.8098\n","Epoch 636/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.8108\n","Epoch 636: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1362 - acc: 0.8107 - val_loss: 0.1375 - val_acc: 0.8105\n","Epoch 637/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.8100\n","Epoch 637: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1363 - acc: 0.8099 - val_loss: 0.1371 - val_acc: 0.8108\n","Epoch 638/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.8102\n","Epoch 638: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1362 - acc: 0.8102 - val_loss: 0.1384 - val_acc: 0.8104\n","Epoch 639/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8102\n","Epoch 639: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8104 - val_loss: 0.1389 - val_acc: 0.8094\n","Epoch 640/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8099\n","Epoch 640: val_loss did not improve from 0.13693\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1362 - acc: 0.8098 - val_loss: 0.1417 - val_acc: 0.8019\n","Epoch 641/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1363 - acc: 0.8097\n","Epoch 641: val_loss improved from 0.13693 to 0.13681, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1364 - acc: 0.8100 - val_loss: 0.1368 - val_acc: 0.8110\n","Epoch 642/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8101\n","Epoch 642: val_loss did not improve from 0.13681\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8102 - val_loss: 0.1369 - val_acc: 0.8076\n","Epoch 643/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8105\n","Epoch 643: val_loss did not improve from 0.13681\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8104 - val_loss: 0.1377 - val_acc: 0.8087\n","Epoch 644/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8104\n","Epoch 644: val_loss did not improve from 0.13681\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8103 - val_loss: 0.1371 - val_acc: 0.8087\n","Epoch 645/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.8100\n","Epoch 645: val_loss did not improve from 0.13681\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1362 - acc: 0.8101 - val_loss: 0.1375 - val_acc: 0.8089\n","Epoch 646/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1361 - acc: 0.8108\n","Epoch 646: val_loss did not improve from 0.13681\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8108 - val_loss: 0.1369 - val_acc: 0.8107\n","Epoch 647/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8104\n","Epoch 647: val_loss did not improve from 0.13681\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8104 - val_loss: 0.1371 - val_acc: 0.8094\n","Epoch 648/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.8103\n","Epoch 648: val_loss improved from 0.13681 to 0.13676, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1362 - acc: 0.8103 - val_loss: 0.1368 - val_acc: 0.8082\n","Epoch 649/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8100\n","Epoch 649: val_loss did not improve from 0.13676\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8100 - val_loss: 0.1372 - val_acc: 0.8094\n","Epoch 650/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1360 - acc: 0.8108\n","Epoch 650: val_loss did not improve from 0.13676\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8108 - val_loss: 0.1371 - val_acc: 0.8080\n","Epoch 651/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8106\n","Epoch 651: val_loss did not improve from 0.13676\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8107 - val_loss: 0.1371 - val_acc: 0.8100\n","Epoch 652/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8102\n","Epoch 652: val_loss did not improve from 0.13676\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8102 - val_loss: 0.1389 - val_acc: 0.8049\n","Epoch 653/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1363 - acc: 0.8099\n","Epoch 653: val_loss did not improve from 0.13676\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1363 - acc: 0.8099 - val_loss: 0.1368 - val_acc: 0.8094\n","Epoch 654/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8102\n","Epoch 654: val_loss did not improve from 0.13676\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8103 - val_loss: 0.1378 - val_acc: 0.8095\n","Epoch 655/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8108\n","Epoch 655: val_loss improved from 0.13676 to 0.13666, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1359 - acc: 0.8108 - val_loss: 0.1367 - val_acc: 0.8094\n","Epoch 656/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8107\n","Epoch 656: val_loss did not improve from 0.13666\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8106 - val_loss: 0.1372 - val_acc: 0.8094\n","Epoch 657/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8107\n","Epoch 657: val_loss did not improve from 0.13666\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8106 - val_loss: 0.1367 - val_acc: 0.8107\n","Epoch 658/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8109\n","Epoch 658: val_loss did not improve from 0.13666\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8108 - val_loss: 0.1379 - val_acc: 0.8060\n","Epoch 659/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8108\n","Epoch 659: val_loss improved from 0.13666 to 0.13665, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1361 - acc: 0.8107 - val_loss: 0.1366 - val_acc: 0.8095\n","Epoch 660/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8107\n","Epoch 660: val_loss did not improve from 0.13665\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8108 - val_loss: 0.1370 - val_acc: 0.8090\n","Epoch 661/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8102\n","Epoch 661: val_loss did not improve from 0.13665\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8103 - val_loss: 0.1373 - val_acc: 0.8093\n","Epoch 662/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8101\n","Epoch 662: val_loss did not improve from 0.13665\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8101 - val_loss: 0.1370 - val_acc: 0.8086\n","Epoch 663/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8103\n","Epoch 663: val_loss did not improve from 0.13665\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8104 - val_loss: 0.1376 - val_acc: 0.8090\n","Epoch 664/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8109\n","Epoch 664: val_loss did not improve from 0.13665\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8108 - val_loss: 0.1369 - val_acc: 0.8117\n","Epoch 665/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1362 - acc: 0.8103\n","Epoch 665: val_loss did not improve from 0.13665\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1362 - acc: 0.8104 - val_loss: 0.1398 - val_acc: 0.8076\n","Epoch 666/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8106\n","Epoch 666: val_loss improved from 0.13665 to 0.13645, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1359 - acc: 0.8106 - val_loss: 0.1365 - val_acc: 0.8112\n","Epoch 667/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8103\n","Epoch 667: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8103 - val_loss: 0.1381 - val_acc: 0.8041\n","Epoch 668/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8108\n","Epoch 668: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8108 - val_loss: 0.1367 - val_acc: 0.8099\n","Epoch 669/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1359 - acc: 0.8105\n","Epoch 669: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8105 - val_loss: 0.1385 - val_acc: 0.8077\n","Epoch 670/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8101\n","Epoch 670: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8101 - val_loss: 0.1375 - val_acc: 0.8057\n","Epoch 671/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8103\n","Epoch 671: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8104 - val_loss: 0.1408 - val_acc: 0.8109\n","Epoch 672/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8105\n","Epoch 672: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1361 - acc: 0.8106 - val_loss: 0.1365 - val_acc: 0.8110\n","Epoch 673/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8108\n","Epoch 673: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8108 - val_loss: 0.1371 - val_acc: 0.8104\n","Epoch 674/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8104\n","Epoch 674: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8105 - val_loss: 0.1365 - val_acc: 0.8062\n","Epoch 675/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8106\n","Epoch 675: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8106 - val_loss: 0.1368 - val_acc: 0.8116\n","Epoch 676/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.8108\n","Epoch 676: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8107 - val_loss: 0.1365 - val_acc: 0.8106\n","Epoch 677/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8110\n","Epoch 677: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8110 - val_loss: 0.1371 - val_acc: 0.8092\n","Epoch 678/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8105\n","Epoch 678: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8105 - val_loss: 0.1368 - val_acc: 0.8088\n","Epoch 679/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8116\n","Epoch 679: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8116 - val_loss: 0.1375 - val_acc: 0.8089\n","Epoch 680/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1360 - acc: 0.8106\n","Epoch 680: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8108 - val_loss: 0.1366 - val_acc: 0.8092\n","Epoch 681/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8110\n","Epoch 681: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8109 - val_loss: 0.1367 - val_acc: 0.8074\n","Epoch 682/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8113\n","Epoch 682: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8113 - val_loss: 0.1368 - val_acc: 0.8099\n","Epoch 683/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8105\n","Epoch 683: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8105 - val_loss: 0.1366 - val_acc: 0.8090\n","Epoch 684/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8113\n","Epoch 684: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8115 - val_loss: 0.1382 - val_acc: 0.8113\n","Epoch 685/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8110\n","Epoch 685: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8111 - val_loss: 0.1373 - val_acc: 0.8079\n","Epoch 686/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8103\n","Epoch 686: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8103 - val_loss: 0.1365 - val_acc: 0.8117\n","Epoch 687/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1360 - acc: 0.8106\n","Epoch 687: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1360 - acc: 0.8106 - val_loss: 0.1372 - val_acc: 0.8082\n","Epoch 688/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1358 - acc: 0.8112\n","Epoch 688: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8112 - val_loss: 0.1368 - val_acc: 0.8087\n","Epoch 689/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8103\n","Epoch 689: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8104 - val_loss: 0.1366 - val_acc: 0.8099\n","Epoch 690/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8112\n","Epoch 690: val_loss did not improve from 0.13645\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8111 - val_loss: 0.1368 - val_acc: 0.8117\n","Epoch 691/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8105\n","Epoch 691: val_loss improved from 0.13645 to 0.13642, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1358 - acc: 0.8105 - val_loss: 0.1364 - val_acc: 0.8125\n","Epoch 692/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8113\n","Epoch 692: val_loss did not improve from 0.13642\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8114 - val_loss: 0.1366 - val_acc: 0.8090\n","Epoch 693/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8112\n","Epoch 693: val_loss did not improve from 0.13642\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8112 - val_loss: 0.1369 - val_acc: 0.8080\n","Epoch 694/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1358 - acc: 0.8107\n","Epoch 694: val_loss did not improve from 0.13642\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8107 - val_loss: 0.1368 - val_acc: 0.8073\n","Epoch 695/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8107\n","Epoch 695: val_loss did not improve from 0.13642\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1359 - acc: 0.8107 - val_loss: 0.1414 - val_acc: 0.8093\n","Epoch 696/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8107\n","Epoch 696: val_loss did not improve from 0.13642\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8108 - val_loss: 0.1373 - val_acc: 0.8098\n","Epoch 697/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8107\n","Epoch 697: val_loss did not improve from 0.13642\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8108 - val_loss: 0.1371 - val_acc: 0.8089\n","Epoch 698/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8110\n","Epoch 698: val_loss did not improve from 0.13642\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8110 - val_loss: 0.1371 - val_acc: 0.8087\n","Epoch 699/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8110\n","Epoch 699: val_loss did not improve from 0.13642\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1358 - acc: 0.8109 - val_loss: 0.1365 - val_acc: 0.8099\n","Epoch 700/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8113\n","Epoch 700: val_loss improved from 0.13642 to 0.13637, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1357 - acc: 0.8112 - val_loss: 0.1364 - val_acc: 0.8099\n","Epoch 701/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8112\n","Epoch 701: val_loss did not improve from 0.13637\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8112 - val_loss: 0.1365 - val_acc: 0.8126\n","Epoch 702/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8109\n","Epoch 702: val_loss did not improve from 0.13637\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8110 - val_loss: 0.1365 - val_acc: 0.8083\n","Epoch 703/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8109\n","Epoch 703: val_loss did not improve from 0.13637\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8109 - val_loss: 0.1387 - val_acc: 0.8090\n","Epoch 704/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1356 - acc: 0.8119\n","Epoch 704: val_loss did not improve from 0.13637\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1356 - acc: 0.8119 - val_loss: 0.1369 - val_acc: 0.8086\n","Epoch 705/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8115\n","Epoch 705: val_loss did not improve from 0.13637\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8115 - val_loss: 0.1369 - val_acc: 0.8088\n","Epoch 706/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1358 - acc: 0.8113\n","Epoch 706: val_loss did not improve from 0.13637\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8113 - val_loss: 0.1364 - val_acc: 0.8119\n","Epoch 707/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8114\n","Epoch 707: val_loss did not improve from 0.13637\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1356 - acc: 0.8114 - val_loss: 0.1377 - val_acc: 0.8109\n","Epoch 708/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8113\n","Epoch 708: val_loss did not improve from 0.13637\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1356 - acc: 0.8113 - val_loss: 0.1370 - val_acc: 0.8099\n","Epoch 709/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8116\n","Epoch 709: val_loss did not improve from 0.13637\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1356 - acc: 0.8116 - val_loss: 0.1369 - val_acc: 0.8117\n","Epoch 710/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8114\n","Epoch 710: val_loss improved from 0.13637 to 0.13625, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1357 - acc: 0.8114 - val_loss: 0.1363 - val_acc: 0.8100\n","Epoch 711/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8111\n","Epoch 711: val_loss did not improve from 0.13625\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1356 - acc: 0.8111 - val_loss: 0.1387 - val_acc: 0.8099\n","Epoch 712/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1359 - acc: 0.8108\n","Epoch 712: val_loss improved from 0.13625 to 0.13617, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1359 - acc: 0.8110 - val_loss: 0.1362 - val_acc: 0.8091\n","Epoch 713/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8112\n","Epoch 713: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1356 - acc: 0.8113 - val_loss: 0.1367 - val_acc: 0.8113\n","Epoch 714/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8117\n","Epoch 714: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1356 - acc: 0.8119 - val_loss: 0.1364 - val_acc: 0.8120\n","Epoch 715/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8117\n","Epoch 715: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1356 - acc: 0.8118 - val_loss: 0.1367 - val_acc: 0.8092\n","Epoch 716/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8117\n","Epoch 716: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8116 - val_loss: 0.1365 - val_acc: 0.8108\n","Epoch 717/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8119\n","Epoch 717: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1355 - acc: 0.8119 - val_loss: 0.1365 - val_acc: 0.8107\n","Epoch 718/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8117\n","Epoch 718: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1355 - acc: 0.8117 - val_loss: 0.1364 - val_acc: 0.8119\n","Epoch 719/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8118\n","Epoch 719: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1356 - acc: 0.8117 - val_loss: 0.1369 - val_acc: 0.8099\n","Epoch 720/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8120\n","Epoch 720: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1354 - acc: 0.8121 - val_loss: 0.1367 - val_acc: 0.8102\n","Epoch 721/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8114\n","Epoch 721: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1357 - acc: 0.8114 - val_loss: 0.1373 - val_acc: 0.8082\n","Epoch 722/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8113\n","Epoch 722: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1355 - acc: 0.8116 - val_loss: 0.1367 - val_acc: 0.8123\n","Epoch 723/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8119\n","Epoch 723: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1357 - acc: 0.8118 - val_loss: 0.1363 - val_acc: 0.8099\n","Epoch 724/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8117\n","Epoch 724: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1356 - acc: 0.8118 - val_loss: 0.1366 - val_acc: 0.8110\n","Epoch 725/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8120\n","Epoch 725: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1356 - acc: 0.8120 - val_loss: 0.1363 - val_acc: 0.8090\n","Epoch 726/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1355 - acc: 0.8118\n","Epoch 726: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1355 - acc: 0.8118 - val_loss: 0.1380 - val_acc: 0.8096\n","Epoch 727/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8121\n","Epoch 727: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1357 - acc: 0.8119 - val_loss: 0.1370 - val_acc: 0.8124\n","Epoch 728/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8119\n","Epoch 728: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1356 - acc: 0.8120 - val_loss: 0.1367 - val_acc: 0.8128\n","Epoch 729/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8120\n","Epoch 729: val_loss did not improve from 0.13617\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1355 - acc: 0.8120 - val_loss: 0.1363 - val_acc: 0.8097\n","Epoch 730/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8117\n","Epoch 730: val_loss improved from 0.13617 to 0.13616, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1355 - acc: 0.8118 - val_loss: 0.1362 - val_acc: 0.8105\n","Epoch 731/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8122\n","Epoch 731: val_loss did not improve from 0.13616\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1355 - acc: 0.8120 - val_loss: 0.1373 - val_acc: 0.8088\n","Epoch 732/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8114\n","Epoch 732: val_loss did not improve from 0.13616\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1357 - acc: 0.8115 - val_loss: 0.1372 - val_acc: 0.8110\n","Epoch 733/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8117\n","Epoch 733: val_loss did not improve from 0.13616\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1357 - acc: 0.8117 - val_loss: 0.1366 - val_acc: 0.8102\n","Epoch 734/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8124\n","Epoch 734: val_loss improved from 0.13616 to 0.13609, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1354 - acc: 0.8124 - val_loss: 0.1361 - val_acc: 0.8113\n","Epoch 735/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8127\n","Epoch 735: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1354 - acc: 0.8126 - val_loss: 0.1376 - val_acc: 0.8099\n","Epoch 736/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8121\n","Epoch 736: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1355 - acc: 0.8121 - val_loss: 0.1369 - val_acc: 0.8094\n","Epoch 737/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8122\n","Epoch 737: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1354 - acc: 0.8122 - val_loss: 0.1366 - val_acc: 0.8101\n","Epoch 738/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1354 - acc: 0.8121\n","Epoch 738: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1354 - acc: 0.8121 - val_loss: 0.1372 - val_acc: 0.8101\n","Epoch 739/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8120\n","Epoch 739: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1357 - acc: 0.8121 - val_loss: 0.1385 - val_acc: 0.8080\n","Epoch 740/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8117\n","Epoch 740: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8117 - val_loss: 0.1362 - val_acc: 0.8112\n","Epoch 741/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8127\n","Epoch 741: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1353 - acc: 0.8127 - val_loss: 0.1366 - val_acc: 0.8107\n","Epoch 742/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1356 - acc: 0.8118\n","Epoch 742: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1356 - acc: 0.8118 - val_loss: 0.1369 - val_acc: 0.8092\n","Epoch 743/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8123\n","Epoch 743: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1353 - acc: 0.8122 - val_loss: 0.1364 - val_acc: 0.8100\n","Epoch 744/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8121\n","Epoch 744: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1355 - acc: 0.8122 - val_loss: 0.1381 - val_acc: 0.8130\n","Epoch 745/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8118\n","Epoch 745: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8119 - val_loss: 0.1369 - val_acc: 0.8104\n","Epoch 746/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1356 - acc: 0.8126\n","Epoch 746: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1356 - acc: 0.8126 - val_loss: 0.1361 - val_acc: 0.8112\n","Epoch 747/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8126\n","Epoch 747: val_loss did not improve from 0.13609\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1354 - acc: 0.8126 - val_loss: 0.1373 - val_acc: 0.8063\n","Epoch 748/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8123\n","Epoch 748: val_loss improved from 0.13609 to 0.13603, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1355 - acc: 0.8124 - val_loss: 0.1360 - val_acc: 0.8121\n","Epoch 749/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8129\n","Epoch 749: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8127 - val_loss: 0.1365 - val_acc: 0.8110\n","Epoch 750/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8128\n","Epoch 750: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1354 - acc: 0.8127 - val_loss: 0.1367 - val_acc: 0.8132\n","Epoch 751/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8122\n","Epoch 751: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8123 - val_loss: 0.1377 - val_acc: 0.8064\n","Epoch 752/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8125\n","Epoch 752: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1356 - acc: 0.8126 - val_loss: 0.1367 - val_acc: 0.8098\n","Epoch 753/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8117\n","Epoch 753: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8119 - val_loss: 0.1365 - val_acc: 0.8106\n","Epoch 754/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8125\n","Epoch 754: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8125 - val_loss: 0.1364 - val_acc: 0.8132\n","Epoch 755/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8125\n","Epoch 755: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1355 - acc: 0.8125 - val_loss: 0.1364 - val_acc: 0.8126\n","Epoch 756/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8127\n","Epoch 756: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8128 - val_loss: 0.1370 - val_acc: 0.8093\n","Epoch 757/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8128\n","Epoch 757: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1353 - acc: 0.8128 - val_loss: 0.1379 - val_acc: 0.8099\n","Epoch 758/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1355 - acc: 0.8122\n","Epoch 758: val_loss improved from 0.13603 to 0.13603, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1355 - acc: 0.8122 - val_loss: 0.1360 - val_acc: 0.8108\n","Epoch 759/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8125\n","Epoch 759: val_loss did not improve from 0.13603\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1353 - acc: 0.8124 - val_loss: 0.1363 - val_acc: 0.8116\n","Epoch 760/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1354 - acc: 0.8126\n","Epoch 760: val_loss improved from 0.13603 to 0.13592, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1354 - acc: 0.8126 - val_loss: 0.1359 - val_acc: 0.8119\n","Epoch 761/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8127\n","Epoch 761: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8127 - val_loss: 0.1365 - val_acc: 0.8112\n","Epoch 762/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1357 - acc: 0.8119\n","Epoch 762: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1356 - acc: 0.8121 - val_loss: 0.1364 - val_acc: 0.8142\n","Epoch 763/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8125\n","Epoch 763: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1353 - acc: 0.8125 - val_loss: 0.1375 - val_acc: 0.8089\n","Epoch 764/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8128\n","Epoch 764: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8129 - val_loss: 0.1367 - val_acc: 0.8101\n","Epoch 765/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8129\n","Epoch 765: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8130 - val_loss: 0.1367 - val_acc: 0.8101\n","Epoch 766/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8126\n","Epoch 766: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1353 - acc: 0.8126 - val_loss: 0.1371 - val_acc: 0.8088\n","Epoch 767/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1352 - acc: 0.8128\n","Epoch 767: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8128 - val_loss: 0.1364 - val_acc: 0.8094\n","Epoch 768/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8129\n","Epoch 768: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8128 - val_loss: 0.1361 - val_acc: 0.8106\n","Epoch 769/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1352 - acc: 0.8137\n","Epoch 769: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8137 - val_loss: 0.1365 - val_acc: 0.8093\n","Epoch 770/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8123\n","Epoch 770: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1354 - acc: 0.8123 - val_loss: 0.1362 - val_acc: 0.8106\n","Epoch 771/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8122\n","Epoch 771: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1353 - acc: 0.8122 - val_loss: 0.1362 - val_acc: 0.8126\n","Epoch 772/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1356 - acc: 0.8122\n","Epoch 772: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1358 - acc: 0.8121 - val_loss: 0.1408 - val_acc: 0.8098\n","Epoch 773/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8130\n","Epoch 773: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8130 - val_loss: 0.1369 - val_acc: 0.8105\n","Epoch 774/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8126\n","Epoch 774: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8126 - val_loss: 0.1382 - val_acc: 0.8077\n","Epoch 775/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1353 - acc: 0.8123\n","Epoch 775: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1353 - acc: 0.8123 - val_loss: 0.1368 - val_acc: 0.8122\n","Epoch 776/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8131\n","Epoch 776: val_loss did not improve from 0.13592\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8130 - val_loss: 0.1372 - val_acc: 0.8072\n","Epoch 777/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8127\n","Epoch 777: val_loss improved from 0.13592 to 0.13590, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1353 - acc: 0.8127 - val_loss: 0.1359 - val_acc: 0.8105\n","Epoch 778/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8126\n","Epoch 778: val_loss improved from 0.13590 to 0.13580, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1353 - acc: 0.8127 - val_loss: 0.1358 - val_acc: 0.8106\n","Epoch 779/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8130\n","Epoch 779: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8130 - val_loss: 0.1376 - val_acc: 0.8105\n","Epoch 780/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8137\n","Epoch 780: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8136 - val_loss: 0.1371 - val_acc: 0.8078\n","Epoch 781/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8126\n","Epoch 781: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1353 - acc: 0.8125 - val_loss: 0.1420 - val_acc: 0.8077\n","Epoch 782/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1354 - acc: 0.8129\n","Epoch 782: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8129 - val_loss: 0.1364 - val_acc: 0.8123\n","Epoch 783/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8128\n","Epoch 783: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8131 - val_loss: 0.1359 - val_acc: 0.8111\n","Epoch 784/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8127\n","Epoch 784: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1353 - acc: 0.8129 - val_loss: 0.1368 - val_acc: 0.8120\n","Epoch 785/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8132\n","Epoch 785: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8132 - val_loss: 0.1362 - val_acc: 0.8124\n","Epoch 786/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8127\n","Epoch 786: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8128 - val_loss: 0.1364 - val_acc: 0.8106\n","Epoch 787/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1352 - acc: 0.8128\n","Epoch 787: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8128 - val_loss: 0.1359 - val_acc: 0.8110\n","Epoch 788/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1352 - acc: 0.8132\n","Epoch 788: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8132 - val_loss: 0.1369 - val_acc: 0.8099\n","Epoch 789/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1352 - acc: 0.8126\n","Epoch 789: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8126 - val_loss: 0.1379 - val_acc: 0.8070\n","Epoch 790/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8126\n","Epoch 790: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1353 - acc: 0.8125 - val_loss: 0.1361 - val_acc: 0.8125\n","Epoch 791/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8127\n","Epoch 791: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8127 - val_loss: 0.1363 - val_acc: 0.8114\n","Epoch 792/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8130\n","Epoch 792: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8130 - val_loss: 0.1363 - val_acc: 0.8111\n","Epoch 793/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8128\n","Epoch 793: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8129 - val_loss: 0.1366 - val_acc: 0.8133\n","Epoch 794/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8132\n","Epoch 794: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8131 - val_loss: 0.1362 - val_acc: 0.8121\n","Epoch 795/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8129\n","Epoch 795: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1353 - acc: 0.8129 - val_loss: 0.1362 - val_acc: 0.8100\n","Epoch 796/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8130\n","Epoch 796: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1351 - acc: 0.8131 - val_loss: 0.1360 - val_acc: 0.8122\n","Epoch 797/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8130\n","Epoch 797: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8131 - val_loss: 0.1374 - val_acc: 0.8117\n","Epoch 798/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8129\n","Epoch 798: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1353 - acc: 0.8129 - val_loss: 0.1359 - val_acc: 0.8122\n","Epoch 799/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8132\n","Epoch 799: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1351 - acc: 0.8133 - val_loss: 0.1361 - val_acc: 0.8119\n","Epoch 800/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1352 - acc: 0.8132\n","Epoch 800: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 10ms/step - loss: 0.1352 - acc: 0.8132 - val_loss: 0.1360 - val_acc: 0.8123\n","Epoch 801/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8132\n","Epoch 801: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8132 - val_loss: 0.1375 - val_acc: 0.8085\n","Epoch 802/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8137\n","Epoch 802: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8138 - val_loss: 0.1358 - val_acc: 0.8124\n","Epoch 803/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8135\n","Epoch 803: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1351 - acc: 0.8134 - val_loss: 0.1361 - val_acc: 0.8126\n","Epoch 804/1000\n","207/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8136\n","Epoch 804: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1351 - acc: 0.8135 - val_loss: 0.1363 - val_acc: 0.8130\n","Epoch 805/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8130\n","Epoch 805: val_loss did not improve from 0.13580\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8130 - val_loss: 0.1359 - val_acc: 0.8139\n","Epoch 806/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8136\n","Epoch 806: val_loss improved from 0.13580 to 0.13562, saving model to best_model_AutoEncoder.h5\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1353 - acc: 0.8135 - val_loss: 0.1356 - val_acc: 0.8116\n","Epoch 807/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8131\n","Epoch 807: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8131 - val_loss: 0.1362 - val_acc: 0.8114\n","Epoch 808/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8138\n","Epoch 808: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8137 - val_loss: 0.1361 - val_acc: 0.8106\n","Epoch 809/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1353 - acc: 0.8133\n","Epoch 809: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1353 - acc: 0.8134 - val_loss: 0.1365 - val_acc: 0.8118\n","Epoch 810/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8128\n","Epoch 810: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8129 - val_loss: 0.1363 - val_acc: 0.8111\n","Epoch 811/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8138\n","Epoch 811: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1350 - acc: 0.8137 - val_loss: 0.1379 - val_acc: 0.8126\n","Epoch 812/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1353 - acc: 0.8130\n","Epoch 812: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1353 - acc: 0.8130 - val_loss: 0.1363 - val_acc: 0.8124\n","Epoch 813/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8130\n","Epoch 813: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8130 - val_loss: 0.1363 - val_acc: 0.8114\n","Epoch 814/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8131\n","Epoch 814: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8131 - val_loss: 0.1368 - val_acc: 0.8108\n","Epoch 815/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8133\n","Epoch 815: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1350 - acc: 0.8133 - val_loss: 0.1361 - val_acc: 0.8092\n","Epoch 816/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8133\n","Epoch 816: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1350 - acc: 0.8134 - val_loss: 0.1369 - val_acc: 0.8108\n","Epoch 817/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8136\n","Epoch 817: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8135 - val_loss: 0.1369 - val_acc: 0.8094\n","Epoch 818/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1354 - acc: 0.8126\n","Epoch 818: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1354 - acc: 0.8125 - val_loss: 0.1369 - val_acc: 0.8115\n","Epoch 819/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8130\n","Epoch 819: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8132 - val_loss: 0.1364 - val_acc: 0.8107\n","Epoch 820/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8138\n","Epoch 820: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1350 - acc: 0.8138 - val_loss: 0.1370 - val_acc: 0.8135\n","Epoch 821/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8136\n","Epoch 821: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8136 - val_loss: 0.1364 - val_acc: 0.8124\n","Epoch 822/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8135\n","Epoch 822: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8135 - val_loss: 0.1370 - val_acc: 0.8097\n","Epoch 823/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8133\n","Epoch 823: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8133 - val_loss: 0.1364 - val_acc: 0.8107\n","Epoch 824/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.8135\n","Epoch 824: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1350 - acc: 0.8137 - val_loss: 0.1364 - val_acc: 0.8115\n","Epoch 825/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8133\n","Epoch 825: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8132 - val_loss: 0.1365 - val_acc: 0.8115\n","Epoch 826/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8133\n","Epoch 826: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1350 - acc: 0.8134 - val_loss: 0.1369 - val_acc: 0.8113\n","Epoch 827/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8130\n","Epoch 827: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8131 - val_loss: 0.1363 - val_acc: 0.8094\n","Epoch 828/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8127\n","Epoch 828: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8127 - val_loss: 0.1361 - val_acc: 0.8107\n","Epoch 829/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8123\n","Epoch 829: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8124 - val_loss: 0.1356 - val_acc: 0.8124\n","Epoch 830/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8131\n","Epoch 830: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1351 - acc: 0.8132 - val_loss: 0.1364 - val_acc: 0.8103\n","Epoch 831/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.8140\n","Epoch 831: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1349 - acc: 0.8139 - val_loss: 0.1362 - val_acc: 0.8104\n","Epoch 832/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8130\n","Epoch 832: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8130 - val_loss: 0.1363 - val_acc: 0.8093\n","Epoch 833/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8138\n","Epoch 833: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8135 - val_loss: 0.1379 - val_acc: 0.8120\n","Epoch 834/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1351 - acc: 0.8136\n","Epoch 834: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8136 - val_loss: 0.1362 - val_acc: 0.8140\n","Epoch 835/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8133\n","Epoch 835: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1351 - acc: 0.8133 - val_loss: 0.1366 - val_acc: 0.8120\n","Epoch 836/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.8130\n","Epoch 836: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1350 - acc: 0.8130 - val_loss: 0.1365 - val_acc: 0.8081\n","Epoch 837/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8132\n","Epoch 837: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1351 - acc: 0.8134 - val_loss: 0.1367 - val_acc: 0.8108\n","Epoch 838/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8131\n","Epoch 838: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8130 - val_loss: 0.1364 - val_acc: 0.8139\n","Epoch 839/1000\n","208/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8130\n","Epoch 839: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1351 - acc: 0.8130 - val_loss: 0.1362 - val_acc: 0.8114\n","Epoch 840/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8137\n","Epoch 840: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1350 - acc: 0.8136 - val_loss: 0.1367 - val_acc: 0.8108\n","Epoch 841/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8124\n","Epoch 841: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8124 - val_loss: 0.1375 - val_acc: 0.8108\n","Epoch 842/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8134\n","Epoch 842: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1350 - acc: 0.8134 - val_loss: 0.1358 - val_acc: 0.8115\n","Epoch 843/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.8133\n","Epoch 843: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1349 - acc: 0.8134 - val_loss: 0.1382 - val_acc: 0.8120\n","Epoch 844/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8136\n","Epoch 844: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1350 - acc: 0.8137 - val_loss: 0.1365 - val_acc: 0.8093\n","Epoch 845/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8136\n","Epoch 845: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1350 - acc: 0.8136 - val_loss: 0.1359 - val_acc: 0.8102\n","Epoch 846/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.8138\n","Epoch 846: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1350 - acc: 0.8138 - val_loss: 0.1379 - val_acc: 0.8086\n","Epoch 847/1000\n","211/214 [============================>.] - ETA: 0s - loss: 0.1352 - acc: 0.8129\n","Epoch 847: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 9ms/step - loss: 0.1352 - acc: 0.8129 - val_loss: 0.1364 - val_acc: 0.8125\n","Epoch 848/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1350 - acc: 0.8134\n","Epoch 848: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1349 - acc: 0.8135 - val_loss: 0.1372 - val_acc: 0.8096\n","Epoch 849/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8136\n","Epoch 849: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8136 - val_loss: 0.1362 - val_acc: 0.8123\n","Epoch 850/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.8138\n","Epoch 850: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1349 - acc: 0.8137 - val_loss: 0.1372 - val_acc: 0.8127\n","Epoch 851/1000\n","213/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8136\n","Epoch 851: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8136 - val_loss: 0.1361 - val_acc: 0.8134\n","Epoch 852/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1352 - acc: 0.8132\n","Epoch 852: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8132 - val_loss: 0.1366 - val_acc: 0.8124\n","Epoch 853/1000\n","210/214 [============================>.] - ETA: 0s - loss: 0.1349 - acc: 0.8138\n","Epoch 853: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1349 - acc: 0.8137 - val_loss: 0.1369 - val_acc: 0.8110\n","Epoch 854/1000\n","212/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8134\n","Epoch 854: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1352 - acc: 0.8134 - val_loss: 0.1360 - val_acc: 0.8129\n","Epoch 855/1000\n","209/214 [============================>.] - ETA: 0s - loss: 0.1351 - acc: 0.8138\n","Epoch 855: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1351 - acc: 0.8138 - val_loss: 0.1362 - val_acc: 0.8131\n","Epoch 856/1000\n","214/214 [==============================] - ETA: 0s - loss: 0.1350 - acc: 0.8138\n","Epoch 856: val_loss did not improve from 0.13562\n","214/214 [==============================] - 2s 8ms/step - loss: 0.1350 - acc: 0.8138 - val_loss: 0.1362 - val_acc: 0.8104\n","Epoch 856: early stopping\n"]}]},{"cell_type":"code","source":["#evaluating the model\n","Denoised = Denoising_autoencoder.predict(noise_signal_val[:10])\n","\n","print(Denoised.shape)\n","print(Denoised - clean_signal_val[:10])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UKEdC1B2RrOG","executionInfo":{"status":"ok","timestamp":1671201147280,"user_tz":-210,"elapsed":18727,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"bdfc85de-4a67-45ff-c10a-a952e02ca4c2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 165ms/step\n","(10, 29)\n","        scaled_amount        V1        V2        V3        V4        V5  \\\n","18456        0.001225 -0.009171 -0.077504 -0.044878 -0.070162 -0.035370   \n","62262        0.000359  0.045271  0.318427  0.042105  0.135709  0.132202   \n","150576      -0.000123  0.083578 -0.062772 -0.034202 -0.029849  0.003076   \n","152002      -0.000699 -0.071464 -0.124730  0.105225 -0.262551 -0.082694   \n","197934       0.001621 -0.201798 -0.152492 -0.301745 -0.123606  0.206779   \n","238493      -0.000541  0.233272 -0.041442  0.397366 -0.208580  0.170981   \n","123170       0.001318  0.175625  0.026160 -0.085856 -0.086044 -0.267816   \n","11910       -0.000431 -0.135725 -0.056358 -0.103302  0.027212  0.144614   \n","266164       0.001150  0.470891 -0.461665 -1.006749 -0.073835  0.432027   \n","64125        0.001219 -0.210002  0.105827  0.308770  0.017126  0.335073   \n","\n","              V6        V7        V8        V9  ...       V19       V20  \\\n","18456  -0.012876 -0.197472 -0.001496 -0.191242  ... -0.575593 -0.007585   \n","62262  -0.048904  0.114734 -0.082246 -0.013763  ...  0.026946 -0.060420   \n","150576  0.076368  0.116066 -0.145651  0.091566  ...  0.490245 -0.052386   \n","152002 -0.309559 -0.271519  0.097114 -0.221668  ...  0.359109  0.137468   \n","197934 -0.192576  0.261763 -0.074475 -0.157243  ...  0.284643  0.202524   \n","238493  0.010271 -0.115393  0.232278  0.144886  ... -0.411902  0.039218   \n","123170 -0.225822 -0.034047 -0.123140 -0.167339  ... -0.020535  0.007844   \n","11910   0.004674  0.004606 -0.023068 -0.219689  ...  0.098131 -0.043364   \n","266164 -0.347358  0.308401 -0.445518  0.046624  ... -0.051010 -0.322412   \n","64125   0.099784  0.034073 -0.015289 -0.006213  ...  0.245506  0.110697   \n","\n","             V21       V22       V23       V24       V25       V26       V27  \\\n","18456   0.045648 -0.090260  0.161383  0.456108 -0.625979  0.501886 -0.047580   \n","62262   0.040198  0.030627 -0.169523  0.322420  0.451144 -0.745068  0.109741   \n","150576 -0.070740 -0.262798 -0.284340 -0.228778  0.518223 -0.691055  0.129951   \n","152002 -0.185813 -0.701460  0.235696 -0.092183 -0.288874  0.168800  0.138004   \n","197934  0.147918 -0.309452 -0.042525 -0.073023  0.442032 -0.154677 -0.283623   \n","238493 -0.250964 -0.101905  0.101327  0.121774 -0.342168  0.491352 -0.128632   \n","123170  0.031946 -0.064615  0.165791 -0.002757 -0.594687  0.800404 -0.068592   \n","11910  -0.152146 -0.572862  0.095179 -0.579096 -0.603762  0.585664  0.010008   \n","266164 -0.107522 -0.244219 -0.063603 -0.024430 -0.338001 -0.583472 -0.074140   \n","64125  -0.005268 -0.108004 -0.118995  0.613779  0.737174 -0.460572  0.097589   \n","\n","             V28  \n","18456   0.106283  \n","62262   0.022321  \n","150576 -0.077818  \n","152002  0.113018  \n","197934 -0.107686  \n","238493  0.153763  \n","123170  0.022708  \n","11910  -0.129269  \n","266164  0.048789  \n","64125  -0.063165  \n","\n","[10 rows x 29 columns]\n"]}]},{"cell_type":"markdown","source":["# Classifier"],"metadata":{"id":"xlOd8UfIWrB2"}},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","Denoising_Autoencoder = load_model('best_model_AutoEncoder.h5')\n","Denoising_Autoencoder.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"osy34Xp6Wqcz","executionInfo":{"status":"ok","timestamp":1671201266145,"user_tz":-210,"elapsed":441,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"b6be7a89-c6a0-42a8-c412-bf3b6901b652"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 29)]              0         \n","                                                                 \n"," dense (Dense)               (None, 22)                660       \n","                                                                 \n"," dense_1 (Dense)             (None, 15)                345       \n","                                                                 \n"," dense_2 (Dense)             (None, 10)                160       \n","                                                                 \n"," dense_3 (Dense)             (None, 15)                165       \n","                                                                 \n"," dense_4 (Dense)             (None, 22)                352       \n","                                                                 \n"," dense_5 (Dense)             (None, 29)                667       \n","                                                                 \n","=================================================================\n","Total params: 2,349\n","Trainable params: 2,349\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# freeze all autoencoder layers\n","for layer in Denoising_Autoencoder.layers:\n","    layer.trainable = False\n","\n","Denoising_Autoencoder.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YCYjD3awW7p-","executionInfo":{"status":"ok","timestamp":1671201283033,"user_tz":-210,"elapsed":347,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"1d8ccb8b-7755-4049-d0c5-61290b88d8bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 29)]              0         \n","                                                                 \n"," dense (Dense)               (None, 22)                660       \n","                                                                 \n"," dense_1 (Dense)             (None, 15)                345       \n","                                                                 \n"," dense_2 (Dense)             (None, 10)                160       \n","                                                                 \n"," dense_3 (Dense)             (None, 15)                165       \n","                                                                 \n"," dense_4 (Dense)             (None, 22)                352       \n","                                                                 \n"," dense_5 (Dense)             (None, 29)                667       \n","                                                                 \n","=================================================================\n","Total params: 2,349\n","Trainable params: 0\n","Non-trainable params: 2,349\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["inp_layer = Input(shape = (29,))\n","denoised = Denoising_Autoencoder(inp_layer)\n","hidden_layer1 = Dense(units = 22, activation = 'relu')(denoised)\n","hidden_layer2 = Dense(units = 15, activation = 'relu')(hidden_layer1)\n","hidden_layer3 = Dense(units = 10, activation = 'relu')(hidden_layer2)\n","hidden_layer4 = Dense(units = 5, activation = 'relu')(hidden_layer3)\n","out_layer = Dense(units = 2, activation = 'softmax')(hidden_layer4)\n","\n","Classifier = Model(inputs = inp_layer, outputs = out_layer)\n","Classifier.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hrbHyWMnXAyG","executionInfo":{"status":"ok","timestamp":1671201713493,"user_tz":-210,"elapsed":490,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"80e71902-6a64-4ced-f8ab-db2e5021b65b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 29)]              0         \n","                                                                 \n"," model (Functional)          (None, 29)                2349      \n","                                                                 \n"," dense_11 (Dense)            (None, 22)                660       \n","                                                                 \n"," dense_12 (Dense)            (None, 15)                345       \n","                                                                 \n"," dense_13 (Dense)            (None, 10)                160       \n","                                                                 \n"," dense_14 (Dense)            (None, 5)                 55        \n","                                                                 \n"," dense_15 (Dense)            (None, 2)                 12        \n","                                                                 \n","=================================================================\n","Total params: 3,581\n","Trainable params: 1,232\n","Non-trainable params: 2,349\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import binary_crossentropy\n","\n","opt = Adam(learning_rate=0.001)\n","loss = binary_crossentropy\n","Classifier.compile(optimizer=opt, loss = loss, metrics = 'acc')"],"metadata":{"id":"4HneNHQtXFZj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import h5py\n","from keras.callbacks import ModelCheckpoint, EarlyStopping\n","\n","es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n","mc = ModelCheckpoint('best_model_withSMOTE.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)"],"metadata":{"id":"iS0rP62MXMs3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(y_train_res.shape)\n","print(X_train_res.shape)\n","print(y_val.shape)\n","print(X_val.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HKCxiu4rXp6g","executionInfo":{"status":"ok","timestamp":1671201467480,"user_tz":-210,"elapsed":545,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"41b41f37-13af-42aa-a0b5-b13698799d1d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(272955,)\n","(272955, 29)\n","(45569, 2)\n","(45569, 29)\n"]}]},{"cell_type":"code","source":["#y_train_res = to_categorical(y_train_res, num_classes = 2)"],"metadata":{"id":"PfYKFK5ZYD1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_Classifier = Classifier.fit(X_train_res, y_train_res,\n","                            batch_size=1024,\n","                            epochs=1000,\n","                            validation_data = (X_val, y_val),\n","                            shuffle=True,\n","                            callbacks=[es, mc])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"syjXq_QfXRqf","executionInfo":{"status":"ok","timestamp":1671201802578,"user_tz":-210,"elapsed":83450,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"f297ac8a-224c-4323-aff0-6ca9f6abdafb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/1000\n","260/267 [============================>.] - ETA: 0s - loss: 0.4260 - acc: 0.8383\n","Epoch 1: val_loss improved from inf to 0.05596, saving model to best_model_withSMOTE.h5\n","267/267 [==============================] - 2s 6ms/step - loss: 0.4195 - acc: 0.8413 - val_loss: 0.0560 - val_acc: 0.9856\n","Epoch 2/1000\n","256/267 [===========================>..] - ETA: 0s - loss: 0.0929 - acc: 0.9651\n","Epoch 2: val_loss improved from 0.05596 to 0.04201, saving model to best_model_withSMOTE.h5\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0920 - acc: 0.9654 - val_loss: 0.0420 - val_acc: 0.9873\n","Epoch 3/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0553 - acc: 0.9787\n","Epoch 3: val_loss improved from 0.04201 to 0.03469, saving model to best_model_withSMOTE.h5\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0553 - acc: 0.9787 - val_loss: 0.0347 - val_acc: 0.9878\n","Epoch 4/1000\n","258/267 [===========================>..] - ETA: 0s - loss: 0.0404 - acc: 0.9861\n","Epoch 4: val_loss improved from 0.03469 to 0.03012, saving model to best_model_withSMOTE.h5\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0401 - acc: 0.9862 - val_loss: 0.0301 - val_acc: 0.9894\n","Epoch 5/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0316 - acc: 0.9898\n","Epoch 5: val_loss improved from 0.03012 to 0.02274, saving model to best_model_withSMOTE.h5\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0316 - acc: 0.9898 - val_loss: 0.0227 - val_acc: 0.9925\n","Epoch 6/1000\n","262/267 [============================>.] - ETA: 0s - loss: 0.0264 - acc: 0.9919\n","Epoch 6: val_loss did not improve from 0.02274\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0263 - acc: 0.9920 - val_loss: 0.0270 - val_acc: 0.9901\n","Epoch 7/1000\n","263/267 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9933\n","Epoch 7: val_loss improved from 0.02274 to 0.01751, saving model to best_model_withSMOTE.h5\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0226 - acc: 0.9933 - val_loss: 0.0175 - val_acc: 0.9945\n","Epoch 8/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0201 - acc: 0.9940\n","Epoch 8: val_loss did not improve from 0.01751\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0201 - acc: 0.9940 - val_loss: 0.0229 - val_acc: 0.9924\n","Epoch 9/1000\n","261/267 [============================>.] - ETA: 0s - loss: 0.0180 - acc: 0.9946\n","Epoch 9: val_loss did not improve from 0.01751\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0180 - acc: 0.9946 - val_loss: 0.0214 - val_acc: 0.9935\n","Epoch 10/1000\n","267/267 [==============================] - ETA: 0s - loss: 0.0162 - acc: 0.9952\n","Epoch 10: val_loss did not improve from 0.01751\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0162 - acc: 0.9952 - val_loss: 0.0209 - val_acc: 0.9938\n","Epoch 11/1000\n","258/267 [===========================>..] - ETA: 0s - loss: 0.0146 - acc: 0.9960\n","Epoch 11: val_loss improved from 0.01751 to 0.01553, saving model to best_model_withSMOTE.h5\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0146 - acc: 0.9960 - val_loss: 0.0155 - val_acc: 0.9960\n","Epoch 12/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0135 - acc: 0.9962\n","Epoch 12: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0135 - acc: 0.9962 - val_loss: 0.0202 - val_acc: 0.9948\n","Epoch 13/1000\n","257/267 [===========================>..] - ETA: 0s - loss: 0.0123 - acc: 0.9967\n","Epoch 13: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0124 - acc: 0.9967 - val_loss: 0.0191 - val_acc: 0.9952\n","Epoch 14/1000\n","264/267 [============================>.] - ETA: 0s - loss: 0.0118 - acc: 0.9969\n","Epoch 14: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0118 - acc: 0.9969 - val_loss: 0.0181 - val_acc: 0.9957\n","Epoch 15/1000\n","258/267 [===========================>..] - ETA: 0s - loss: 0.0109 - acc: 0.9973\n","Epoch 15: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0109 - acc: 0.9973 - val_loss: 0.0177 - val_acc: 0.9959\n","Epoch 16/1000\n","259/267 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9973\n","Epoch 16: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0105 - acc: 0.9973 - val_loss: 0.0194 - val_acc: 0.9952\n","Epoch 17/1000\n","256/267 [===========================>..] - ETA: 0s - loss: 0.0105 - acc: 0.9973\n","Epoch 17: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0105 - acc: 0.9973 - val_loss: 0.0174 - val_acc: 0.9963\n","Epoch 18/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0096 - acc: 0.9977\n","Epoch 18: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0096 - acc: 0.9977 - val_loss: 0.0192 - val_acc: 0.9958\n","Epoch 19/1000\n","263/267 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.9977\n","Epoch 19: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0093 - acc: 0.9977 - val_loss: 0.0202 - val_acc: 0.9957\n","Epoch 20/1000\n","258/267 [===========================>..] - ETA: 0s - loss: 0.0090 - acc: 0.9978\n","Epoch 20: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0089 - acc: 0.9978 - val_loss: 0.0174 - val_acc: 0.9965\n","Epoch 21/1000\n","267/267 [==============================] - ETA: 0s - loss: 0.0085 - acc: 0.9979\n","Epoch 21: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0085 - acc: 0.9979 - val_loss: 0.0180 - val_acc: 0.9965\n","Epoch 22/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0082 - acc: 0.9980\n","Epoch 22: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0082 - acc: 0.9980 - val_loss: 0.0178 - val_acc: 0.9966\n","Epoch 23/1000\n","260/267 [============================>.] - ETA: 0s - loss: 0.0083 - acc: 0.9980\n","Epoch 23: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0083 - acc: 0.9980 - val_loss: 0.0184 - val_acc: 0.9965\n","Epoch 24/1000\n","267/267 [==============================] - ETA: 0s - loss: 0.0078 - acc: 0.9982\n","Epoch 24: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0078 - acc: 0.9982 - val_loss: 0.0203 - val_acc: 0.9960\n","Epoch 25/1000\n","262/267 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.9981\n","Epoch 25: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0079 - acc: 0.9981 - val_loss: 0.0187 - val_acc: 0.9970\n","Epoch 26/1000\n","258/267 [===========================>..] - ETA: 0s - loss: 0.0073 - acc: 0.9983\n","Epoch 26: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0074 - acc: 0.9983 - val_loss: 0.0189 - val_acc: 0.9968\n","Epoch 27/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9983\n","Epoch 27: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0073 - acc: 0.9983 - val_loss: 0.0207 - val_acc: 0.9964\n","Epoch 28/1000\n","262/267 [============================>.] - ETA: 0s - loss: 0.0073 - acc: 0.9983\n","Epoch 28: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0073 - acc: 0.9983 - val_loss: 0.0178 - val_acc: 0.9972\n","Epoch 29/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0069 - acc: 0.9984\n","Epoch 29: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0069 - acc: 0.9984 - val_loss: 0.0189 - val_acc: 0.9970\n","Epoch 30/1000\n","265/267 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9985\n","Epoch 30: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0066 - acc: 0.9985 - val_loss: 0.0181 - val_acc: 0.9973\n","Epoch 31/1000\n","262/267 [============================>.] - ETA: 0s - loss: 0.0068 - acc: 0.9984\n","Epoch 31: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0068 - acc: 0.9984 - val_loss: 0.0197 - val_acc: 0.9971\n","Epoch 32/1000\n","261/267 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9983\n","Epoch 32: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0070 - acc: 0.9983 - val_loss: 0.0213 - val_acc: 0.9966\n","Epoch 33/1000\n","259/267 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9983\n","Epoch 33: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0070 - acc: 0.9983 - val_loss: 0.0189 - val_acc: 0.9973\n","Epoch 34/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0064 - acc: 0.9985\n","Epoch 34: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0064 - acc: 0.9985 - val_loss: 0.0187 - val_acc: 0.9976\n","Epoch 35/1000\n","260/267 [============================>.] - ETA: 0s - loss: 0.0066 - acc: 0.9984\n","Epoch 35: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0066 - acc: 0.9984 - val_loss: 0.0211 - val_acc: 0.9967\n","Epoch 36/1000\n","265/267 [============================>.] - ETA: 0s - loss: 0.0062 - acc: 0.9985\n","Epoch 36: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0062 - acc: 0.9985 - val_loss: 0.0189 - val_acc: 0.9976\n","Epoch 37/1000\n","257/267 [===========================>..] - ETA: 0s - loss: 0.0061 - acc: 0.9985\n","Epoch 37: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.0206 - val_acc: 0.9971\n","Epoch 38/1000\n","259/267 [============================>.] - ETA: 0s - loss: 0.0060 - acc: 0.9986\n","Epoch 38: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0059 - acc: 0.9986 - val_loss: 0.0222 - val_acc: 0.9965\n","Epoch 39/1000\n","267/267 [==============================] - ETA: 0s - loss: 0.0059 - acc: 0.9985\n","Epoch 39: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.0189 - val_acc: 0.9974\n","Epoch 40/1000\n","267/267 [==============================] - ETA: 0s - loss: 0.0059 - acc: 0.9986\n","Epoch 40: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0059 - acc: 0.9986 - val_loss: 0.0187 - val_acc: 0.9977\n","Epoch 41/1000\n","260/267 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9987\n","Epoch 41: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0055 - acc: 0.9987 - val_loss: 0.0196 - val_acc: 0.9975\n","Epoch 42/1000\n","260/267 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9987\n","Epoch 42: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.0180 - val_acc: 0.9980\n","Epoch 43/1000\n","260/267 [============================>.] - ETA: 0s - loss: 0.0053 - acc: 0.9988\n","Epoch 43: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.0231 - val_acc: 0.9966\n","Epoch 44/1000\n","260/267 [============================>.] - ETA: 0s - loss: 0.0054 - acc: 0.9987\n","Epoch 44: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0054 - acc: 0.9987 - val_loss: 0.0193 - val_acc: 0.9978\n","Epoch 45/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9988\n","Epoch 45: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.0192 - val_acc: 0.9977\n","Epoch 46/1000\n","260/267 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9987\n","Epoch 46: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0195 - val_acc: 0.9979\n","Epoch 47/1000\n","265/267 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9988\n","Epoch 47: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.0227 - val_acc: 0.9968\n","Epoch 48/1000\n","265/267 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9987\n","Epoch 48: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0052 - acc: 0.9987 - val_loss: 0.0193 - val_acc: 0.9979\n","Epoch 49/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9988\n","Epoch 49: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.0196 - val_acc: 0.9979\n","Epoch 50/1000\n","263/267 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9990\n","Epoch 50: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.0206 - val_acc: 0.9976\n","Epoch 51/1000\n","260/267 [============================>.] - ETA: 0s - loss: 0.0047 - acc: 0.9988\n","Epoch 51: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0224 - val_acc: 0.9973\n","Epoch 52/1000\n","261/267 [============================>.] - ETA: 0s - loss: 0.0046 - acc: 0.9989\n","Epoch 52: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0046 - acc: 0.9989 - val_loss: 0.0216 - val_acc: 0.9974\n","Epoch 53/1000\n","258/267 [===========================>..] - ETA: 0s - loss: 0.0047 - acc: 0.9989\n","Epoch 53: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0047 - acc: 0.9989 - val_loss: 0.0233 - val_acc: 0.9968\n","Epoch 54/1000\n","264/267 [============================>.] - ETA: 0s - loss: 0.0051 - acc: 0.9988\n","Epoch 54: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0051 - acc: 0.9988 - val_loss: 0.0195 - val_acc: 0.9983\n","Epoch 55/1000\n","266/267 [============================>.] - ETA: 0s - loss: 0.0050 - acc: 0.9988\n","Epoch 55: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0050 - acc: 0.9988 - val_loss: 0.0212 - val_acc: 0.9976\n","Epoch 56/1000\n","262/267 [============================>.] - ETA: 0s - loss: 0.0044 - acc: 0.9989\n","Epoch 56: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0044 - acc: 0.9989 - val_loss: 0.0203 - val_acc: 0.9979\n","Epoch 57/1000\n","262/267 [============================>.] - ETA: 0s - loss: 0.0042 - acc: 0.9990\n","Epoch 57: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.0262 - val_acc: 0.9961\n","Epoch 58/1000\n","261/267 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n","Epoch 58: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.0211 - val_acc: 0.9978\n","Epoch 59/1000\n","261/267 [============================>.] - ETA: 0s - loss: 0.0041 - acc: 0.9990\n","Epoch 59: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.0201 - val_acc: 0.9982\n","Epoch 60/1000\n","261/267 [============================>.] - ETA: 0s - loss: 0.0043 - acc: 0.9990\n","Epoch 60: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0043 - acc: 0.9990 - val_loss: 0.0200 - val_acc: 0.9981\n","Epoch 61/1000\n","258/267 [===========================>..] - ETA: 0s - loss: 0.0038 - acc: 0.9991\n","Epoch 61: val_loss did not improve from 0.01553\n","267/267 [==============================] - 1s 5ms/step - loss: 0.0038 - acc: 0.9991 - val_loss: 0.0210 - val_acc: 0.9981\n","Epoch 61: early stopping\n"]}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","train_loss = results_Classifier.history['loss']\n","val_loss = results_Classifier.history['val_loss']\n","\n","plt.semilogy(train_loss)\n","plt.semilogy(val_loss)\n","\n","plt.legend(['training loss', 'validation loss'])\n","plt.xlabel('epochs')\n","plt.ylabel('CCE')\n","plt.show()\n","\n","accuracy = results_Classifier.history['acc']\n","val_accuracy = results_Classifier.history['val_acc']\n","\n","plt.plot(accuracy)\n","plt.plot(val_accuracy)\n","\n","plt.legend(['training accuracy', 'validation accuracy'])\n","plt.xlabel('epochs')\n","plt.ylabel('Accuracy')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":541},"id":"R2qAsDysXaPG","executionInfo":{"status":"ok","timestamp":1671201862911,"user_tz":-210,"elapsed":36884,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"ea9059d5-41a0-4eef-f70f-51c338bd615e"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["# evaluation\n"],"metadata":{"id":"BqNlwNrsZSnL"}},{"cell_type":"code","source":["from tensorflow.keras.models import load_model\n","Classifier = load_model('best_model_withSMOTE.h5')\n","Classifier.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKaP960LZUgC","executionInfo":{"status":"ok","timestamp":1671285405618,"user_tz":-210,"elapsed":1331,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"f514b6b0-dbc8-4189-f8d0-dc4d301bd4ff"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 29)]              0         \n","                                                                 \n"," model (Functional)          (None, 29)                2349      \n","                                                                 \n"," dense_11 (Dense)            (None, 22)                660       \n","                                                                 \n"," dense_12 (Dense)            (None, 15)                345       \n","                                                                 \n"," dense_13 (Dense)            (None, 10)                160       \n","                                                                 \n"," dense_14 (Dense)            (None, 5)                 55        \n","                                                                 \n"," dense_15 (Dense)            (None, 2)                 12        \n","                                                                 \n","=================================================================\n","Total params: 3,581\n","Trainable params: 1,232\n","Non-trainable params: 2,349\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["y_hat = np.argmax(Classifier.predict(X_test),axis=1)\n","test_labels = np.argmax(y_test, axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ScwB9PeZbCP","executionInfo":{"status":"ok","timestamp":1671202014410,"user_tz":-210,"elapsed":5683,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"a7f8cf14-2cb9-477c-bfdf-7444efd22a2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1781/1781 [==============================] - 3s 2ms/step\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import classification_report\n","print(classification_report(test_labels, y_hat))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kVzuKtpsZwTT","executionInfo":{"status":"ok","timestamp":1671202018362,"user_tz":-210,"elapsed":1081,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"e4df7b45-44e5-433c-ae88-7315bb308e8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00     56861\n","           1       0.31      0.87      0.45       101\n","\n","    accuracy                           1.00     56962\n","   macro avg       0.65      0.93      0.73     56962\n","weighted avg       1.00      1.00      1.00     56962\n","\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay\n","ConfusionMatrixDisplay.from_predictions(test_labels, y_hat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"id":"eNKkIjx8ZdDu","executionInfo":{"status":"ok","timestamp":1671201938665,"user_tz":-210,"elapsed":11,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"00fd3b63-13b6-4f3a-d5a7-7ca874cda5ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7fd339a3ad00>"]},"metadata":{},"execution_count":47},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["y_hat = Classifier.predict(X_test)[:, 1]\n","test_labels = np.argmax(y_test, axis=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SZHdZTXuZi6K","executionInfo":{"status":"ok","timestamp":1671285411758,"user_tz":-210,"elapsed":3066,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"d7d30c71-baa7-4bba-dfd9-b9c5abc95a64"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["1781/1781 [==============================] - 2s 982us/step\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import confusion_matrix\n","\n","x=500\n","AC=[]\n","REC=[]\n","TH=[]\n","for t in range(x):\n","    threshold = (t+1)/(x+1)\n","    #print(threshold)\n","    y_hat_int = (y_hat > threshold).astype(int)\n","    c_matrix = confusion_matrix(test_labels, y_hat_int)\n","    acc = (c_matrix[0,0]+c_matrix[1,1])/(sum(sum(c_matrix)))\n","    recall = c_matrix[1,1]/(c_matrix[1,1]+c_matrix[1,0])\n","    #print(acc)\n","    #print(recall)\n","    AC.append(acc)\n","    REC.append(recall)\n","    TH.append(threshold)"],"metadata":{"id":"O-k9kbboZlrg","executionInfo":{"status":"ok","timestamp":1671285414319,"user_tz":-210,"elapsed":2569,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","plt.plot(TH,AC)\n","plt.xlabel('Threhold')\n","plt.ylabel('Accuracy')\n","plt.show()\n","\n","plt.plot(TH,REC)\n","plt.xlabel('Threhold')\n","plt.ylabel('Recall')\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":541},"id":"ifrpM1E1X68o","executionInfo":{"status":"ok","timestamp":1671285422811,"user_tz":-210,"elapsed":814,"user":{"displayName":"arman forouzesh","userId":"10889948706723544277"}},"outputId":"a0ee7586-2794-4908-cb06-683535ef07cb"},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}